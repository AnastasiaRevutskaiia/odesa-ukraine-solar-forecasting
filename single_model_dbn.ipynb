{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\n",
      "\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "odesa_1\n",
      "./data/Odesa_solar_irradiance_2020_2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GridSearch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_actual= {}\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "forecaster= init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "SupervisedDBNRegression()\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.356292\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.393009\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.406344\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.401138\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.392066\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.378699\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.363753\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.344759\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.331186\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.318232\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.488513\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.478467\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.476560\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.473963\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.471498\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.468560\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.466229\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.463388\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.460459\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.457231\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.067181\n",
      ">> Epoch 2 finished \tANN training loss 0.065347\n",
      ">> Epoch 3 finished \tANN training loss 0.064472\n",
      ">> Epoch 4 finished \tANN training loss 0.063612\n",
      ">> Epoch 5 finished \tANN training loss 0.062763\n",
      ">> Epoch 6 finished \tANN training loss 0.061963\n",
      ">> Epoch 7 finished \tANN training loss 0.061171\n",
      ">> Epoch 8 finished \tANN training loss 0.060378\n",
      ">> Epoch 9 finished \tANN training loss 0.059640\n",
      ">> Epoch 10 finished \tANN training loss 0.058901\n",
      ">> Epoch 11 finished \tANN training loss 0.058151\n",
      ">> Epoch 12 finished \tANN training loss 0.057445\n",
      ">> Epoch 13 finished \tANN training loss 0.056774\n",
      ">> Epoch 14 finished \tANN training loss 0.056088\n",
      ">> Epoch 15 finished \tANN training loss 0.055428\n",
      ">> Epoch 16 finished \tANN training loss 0.054769\n",
      ">> Epoch 17 finished \tANN training loss 0.054138\n",
      ">> Epoch 18 finished \tANN training loss 0.053515\n",
      ">> Epoch 19 finished \tANN training loss 0.052898\n",
      ">> Epoch 20 finished \tANN training loss 0.052302\n",
      ">> Epoch 21 finished \tANN training loss 0.051707\n",
      ">> Epoch 22 finished \tANN training loss 0.051140\n",
      ">> Epoch 23 finished \tANN training loss 0.050573\n",
      ">> Epoch 24 finished \tANN training loss 0.050027\n",
      ">> Epoch 25 finished \tANN training loss 0.049481\n",
      ">> Epoch 26 finished \tANN training loss 0.048931\n",
      ">> Epoch 27 finished \tANN training loss 0.048412\n",
      ">> Epoch 28 finished \tANN training loss 0.047921\n",
      ">> Epoch 29 finished \tANN training loss 0.047411\n",
      ">> Epoch 30 finished \tANN training loss 0.046923\n",
      ">> Epoch 31 finished \tANN training loss 0.046433\n",
      ">> Epoch 32 finished \tANN training loss 0.045964\n",
      ">> Epoch 33 finished \tANN training loss 0.045491\n",
      ">> Epoch 34 finished \tANN training loss 0.045041\n",
      ">> Epoch 35 finished \tANN training loss 0.044590\n",
      ">> Epoch 36 finished \tANN training loss 0.044130\n",
      ">> Epoch 37 finished \tANN training loss 0.043715\n",
      ">> Epoch 38 finished \tANN training loss 0.043299\n",
      ">> Epoch 39 finished \tANN training loss 0.042869\n",
      ">> Epoch 40 finished \tANN training loss 0.042461\n",
      ">> Epoch 41 finished \tANN training loss 0.042067\n",
      ">> Epoch 42 finished \tANN training loss 0.041673\n",
      ">> Epoch 43 finished \tANN training loss 0.041287\n",
      ">> Epoch 44 finished \tANN training loss 0.040914\n",
      ">> Epoch 45 finished \tANN training loss 0.040525\n",
      ">> Epoch 46 finished \tANN training loss 0.040170\n",
      ">> Epoch 47 finished \tANN training loss 0.039803\n",
      ">> Epoch 48 finished \tANN training loss 0.039458\n",
      ">> Epoch 49 finished \tANN training loss 0.039111\n",
      ">> Epoch 50 finished \tANN training loss 0.038773\n",
      ">> Epoch 51 finished \tANN training loss 0.038437\n",
      ">> Epoch 52 finished \tANN training loss 0.038103\n",
      ">> Epoch 53 finished \tANN training loss 0.037799\n",
      ">> Epoch 54 finished \tANN training loss 0.037479\n",
      ">> Epoch 55 finished \tANN training loss 0.037157\n",
      ">> Epoch 56 finished \tANN training loss 0.036866\n",
      ">> Epoch 57 finished \tANN training loss 0.036571\n",
      ">> Epoch 58 finished \tANN training loss 0.036285\n",
      ">> Epoch 59 finished \tANN training loss 0.035992\n",
      ">> Epoch 60 finished \tANN training loss 0.035711\n",
      ">> Epoch 61 finished \tANN training loss 0.035429\n",
      ">> Epoch 62 finished \tANN training loss 0.035170\n",
      ">> Epoch 63 finished \tANN training loss 0.034905\n",
      ">> Epoch 64 finished \tANN training loss 0.034635\n",
      ">> Epoch 65 finished \tANN training loss 0.034388\n",
      ">> Epoch 66 finished \tANN training loss 0.034137\n",
      ">> Epoch 67 finished \tANN training loss 0.033890\n",
      ">> Epoch 68 finished \tANN training loss 0.033650\n",
      ">> Epoch 69 finished \tANN training loss 0.033408\n",
      ">> Epoch 70 finished \tANN training loss 0.033187\n",
      ">> Epoch 71 finished \tANN training loss 0.032958\n",
      ">> Epoch 72 finished \tANN training loss 0.032739\n",
      ">> Epoch 73 finished \tANN training loss 0.032522\n",
      ">> Epoch 74 finished \tANN training loss 0.032301\n",
      ">> Epoch 75 finished \tANN training loss 0.032095\n",
      ">> Epoch 76 finished \tANN training loss 0.031879\n",
      ">> Epoch 77 finished \tANN training loss 0.031684\n",
      ">> Epoch 78 finished \tANN training loss 0.031498\n",
      ">> Epoch 79 finished \tANN training loss 0.031300\n",
      ">> Epoch 80 finished \tANN training loss 0.031112\n",
      ">> Epoch 81 finished \tANN training loss 0.030919\n",
      ">> Epoch 82 finished \tANN training loss 0.030724\n",
      ">> Epoch 83 finished \tANN training loss 0.030563\n",
      ">> Epoch 84 finished \tANN training loss 0.030383\n",
      ">> Epoch 85 finished \tANN training loss 0.030214\n",
      ">> Epoch 86 finished \tANN training loss 0.030041\n",
      ">> Epoch 87 finished \tANN training loss 0.029876\n",
      ">> Epoch 88 finished \tANN training loss 0.029714\n",
      ">> Epoch 89 finished \tANN training loss 0.029558\n",
      ">> Epoch 90 finished \tANN training loss 0.029398\n",
      ">> Epoch 91 finished \tANN training loss 0.029241\n",
      ">> Epoch 92 finished \tANN training loss 0.029093\n",
      ">> Epoch 93 finished \tANN training loss 0.028938\n",
      ">> Epoch 94 finished \tANN training loss 0.028790\n",
      ">> Epoch 95 finished \tANN training loss 0.028637\n",
      ">> Epoch 96 finished \tANN training loss 0.028516\n",
      ">> Epoch 97 finished \tANN training loss 0.028370\n",
      ">> Epoch 98 finished \tANN training loss 0.028235\n",
      ">> Epoch 99 finished \tANN training loss 0.028107\n",
      ">> Epoch 100 finished \tANN training loss 0.027975\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.366669\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.413334\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.438009\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.438576\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.428704\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.416167\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.401849\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.387306\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.372359\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.356121\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.461310\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.450525\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.448443\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.445831\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.442742\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.439942\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.436929\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.433813\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.430352\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.426861\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.060162\n",
      ">> Epoch 2 finished \tANN training loss 0.058670\n",
      ">> Epoch 3 finished \tANN training loss 0.057701\n",
      ">> Epoch 4 finished \tANN training loss 0.056790\n",
      ">> Epoch 5 finished \tANN training loss 0.055885\n",
      ">> Epoch 6 finished \tANN training loss 0.055033\n",
      ">> Epoch 7 finished \tANN training loss 0.054180\n",
      ">> Epoch 8 finished \tANN training loss 0.053361\n",
      ">> Epoch 9 finished \tANN training loss 0.052535\n",
      ">> Epoch 10 finished \tANN training loss 0.051760\n",
      ">> Epoch 11 finished \tANN training loss 0.051008\n",
      ">> Epoch 12 finished \tANN training loss 0.050267\n",
      ">> Epoch 13 finished \tANN training loss 0.049532\n",
      ">> Epoch 14 finished \tANN training loss 0.048833\n",
      ">> Epoch 15 finished \tANN training loss 0.048145\n",
      ">> Epoch 16 finished \tANN training loss 0.047482\n",
      ">> Epoch 17 finished \tANN training loss 0.046823\n",
      ">> Epoch 18 finished \tANN training loss 0.046179\n",
      ">> Epoch 19 finished \tANN training loss 0.045556\n",
      ">> Epoch 20 finished \tANN training loss 0.044944\n",
      ">> Epoch 21 finished \tANN training loss 0.044350\n",
      ">> Epoch 22 finished \tANN training loss 0.043765\n",
      ">> Epoch 23 finished \tANN training loss 0.043207\n",
      ">> Epoch 24 finished \tANN training loss 0.042649\n",
      ">> Epoch 25 finished \tANN training loss 0.042118\n",
      ">> Epoch 26 finished \tANN training loss 0.041583\n",
      ">> Epoch 27 finished \tANN training loss 0.041076\n",
      ">> Epoch 28 finished \tANN training loss 0.040558\n",
      ">> Epoch 29 finished \tANN training loss 0.040070\n",
      ">> Epoch 30 finished \tANN training loss 0.039580\n",
      ">> Epoch 31 finished \tANN training loss 0.039129\n",
      ">> Epoch 32 finished \tANN training loss 0.038661\n",
      ">> Epoch 33 finished \tANN training loss 0.038233\n",
      ">> Epoch 34 finished \tANN training loss 0.037790\n",
      ">> Epoch 35 finished \tANN training loss 0.037373\n",
      ">> Epoch 36 finished \tANN training loss 0.036945\n",
      ">> Epoch 37 finished \tANN training loss 0.036552\n",
      ">> Epoch 38 finished \tANN training loss 0.036142\n",
      ">> Epoch 39 finished \tANN training loss 0.035777\n",
      ">> Epoch 40 finished \tANN training loss 0.035390\n",
      ">> Epoch 41 finished \tANN training loss 0.035022\n",
      ">> Epoch 42 finished \tANN training loss 0.034650\n",
      ">> Epoch 43 finished \tANN training loss 0.034321\n",
      ">> Epoch 44 finished \tANN training loss 0.033972\n",
      ">> Epoch 45 finished \tANN training loss 0.033644\n",
      ">> Epoch 46 finished \tANN training loss 0.033313\n",
      ">> Epoch 47 finished \tANN training loss 0.032992\n",
      ">> Epoch 48 finished \tANN training loss 0.032685\n",
      ">> Epoch 49 finished \tANN training loss 0.032385\n",
      ">> Epoch 50 finished \tANN training loss 0.032094\n",
      ">> Epoch 51 finished \tANN training loss 0.031794\n",
      ">> Epoch 52 finished \tANN training loss 0.031516\n",
      ">> Epoch 53 finished \tANN training loss 0.031242\n",
      ">> Epoch 54 finished \tANN training loss 0.030966\n",
      ">> Epoch 55 finished \tANN training loss 0.030708\n",
      ">> Epoch 56 finished \tANN training loss 0.030437\n",
      ">> Epoch 57 finished \tANN training loss 0.030203\n",
      ">> Epoch 58 finished \tANN training loss 0.029961\n",
      ">> Epoch 59 finished \tANN training loss 0.029713\n",
      ">> Epoch 60 finished \tANN training loss 0.029491\n",
      ">> Epoch 61 finished \tANN training loss 0.029267\n",
      ">> Epoch 62 finished \tANN training loss 0.029047\n",
      ">> Epoch 63 finished \tANN training loss 0.028828\n",
      ">> Epoch 64 finished \tANN training loss 0.028616\n",
      ">> Epoch 65 finished \tANN training loss 0.028413\n",
      ">> Epoch 66 finished \tANN training loss 0.028205\n",
      ">> Epoch 67 finished \tANN training loss 0.028016\n",
      ">> Epoch 68 finished \tANN training loss 0.027823\n",
      ">> Epoch 69 finished \tANN training loss 0.027634\n",
      ">> Epoch 70 finished \tANN training loss 0.027451\n",
      ">> Epoch 71 finished \tANN training loss 0.027271\n",
      ">> Epoch 72 finished \tANN training loss 0.027096\n",
      ">> Epoch 73 finished \tANN training loss 0.026917\n",
      ">> Epoch 74 finished \tANN training loss 0.026760\n",
      ">> Epoch 75 finished \tANN training loss 0.026599\n",
      ">> Epoch 76 finished \tANN training loss 0.026443\n",
      ">> Epoch 77 finished \tANN training loss 0.026277\n",
      ">> Epoch 78 finished \tANN training loss 0.026133\n",
      ">> Epoch 79 finished \tANN training loss 0.025976\n",
      ">> Epoch 80 finished \tANN training loss 0.025835\n",
      ">> Epoch 81 finished \tANN training loss 0.025690\n",
      ">> Epoch 82 finished \tANN training loss 0.025555\n",
      ">> Epoch 83 finished \tANN training loss 0.025417\n",
      ">> Epoch 84 finished \tANN training loss 0.025276\n",
      ">> Epoch 85 finished \tANN training loss 0.025149\n",
      ">> Epoch 86 finished \tANN training loss 0.025019\n",
      ">> Epoch 87 finished \tANN training loss 0.024896\n",
      ">> Epoch 88 finished \tANN training loss 0.024769\n",
      ">> Epoch 89 finished \tANN training loss 0.024650\n",
      ">> Epoch 90 finished \tANN training loss 0.024532\n",
      ">> Epoch 91 finished \tANN training loss 0.024415\n",
      ">> Epoch 92 finished \tANN training loss 0.024301\n",
      ">> Epoch 93 finished \tANN training loss 0.024191\n",
      ">> Epoch 94 finished \tANN training loss 0.024082\n",
      ">> Epoch 95 finished \tANN training loss 0.023974\n",
      ">> Epoch 96 finished \tANN training loss 0.023865\n",
      ">> Epoch 97 finished \tANN training loss 0.023763\n",
      ">> Epoch 98 finished \tANN training loss 0.023664\n",
      ">> Epoch 99 finished \tANN training loss 0.023564\n",
      ">> Epoch 100 finished \tANN training loss 0.023463\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.390440\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.435313\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.455900\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.452027\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.442842\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.428660\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.413566\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.399290\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.383502\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.365682\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.445044\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.433943\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.432044\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.429677\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.427222\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.424486\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.421956\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.419100\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.416131\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.413072\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.075744\n",
      ">> Epoch 2 finished \tANN training loss 0.053379\n",
      ">> Epoch 3 finished \tANN training loss 0.052748\n",
      ">> Epoch 4 finished \tANN training loss 0.052161\n",
      ">> Epoch 5 finished \tANN training loss 0.051573\n",
      ">> Epoch 6 finished \tANN training loss 0.050975\n",
      ">> Epoch 7 finished \tANN training loss 0.050404\n",
      ">> Epoch 8 finished \tANN training loss 0.049838\n",
      ">> Epoch 9 finished \tANN training loss 0.049295\n",
      ">> Epoch 10 finished \tANN training loss 0.048746\n",
      ">> Epoch 11 finished \tANN training loss 0.048227\n",
      ">> Epoch 12 finished \tANN training loss 0.047692\n",
      ">> Epoch 13 finished \tANN training loss 0.047178\n",
      ">> Epoch 14 finished \tANN training loss 0.046643\n",
      ">> Epoch 15 finished \tANN training loss 0.046158\n",
      ">> Epoch 16 finished \tANN training loss 0.045682\n",
      ">> Epoch 17 finished \tANN training loss 0.045199\n",
      ">> Epoch 18 finished \tANN training loss 0.044715\n",
      ">> Epoch 19 finished \tANN training loss 0.044248\n",
      ">> Epoch 20 finished \tANN training loss 0.043776\n",
      ">> Epoch 21 finished \tANN training loss 0.043331\n",
      ">> Epoch 22 finished \tANN training loss 0.042881\n",
      ">> Epoch 23 finished \tANN training loss 0.042440\n",
      ">> Epoch 24 finished \tANN training loss 0.042012\n",
      ">> Epoch 25 finished \tANN training loss 0.041585\n",
      ">> Epoch 26 finished \tANN training loss 0.041153\n",
      ">> Epoch 27 finished \tANN training loss 0.040764\n",
      ">> Epoch 28 finished \tANN training loss 0.040356\n",
      ">> Epoch 29 finished \tANN training loss 0.039956\n",
      ">> Epoch 30 finished \tANN training loss 0.039563\n",
      ">> Epoch 31 finished \tANN training loss 0.039173\n",
      ">> Epoch 32 finished \tANN training loss 0.038782\n",
      ">> Epoch 33 finished \tANN training loss 0.038420\n",
      ">> Epoch 34 finished \tANN training loss 0.038063\n",
      ">> Epoch 35 finished \tANN training loss 0.037701\n",
      ">> Epoch 36 finished \tANN training loss 0.037344\n",
      ">> Epoch 37 finished \tANN training loss 0.036998\n",
      ">> Epoch 38 finished \tANN training loss 0.036652\n",
      ">> Epoch 39 finished \tANN training loss 0.036316\n",
      ">> Epoch 40 finished \tANN training loss 0.035991\n",
      ">> Epoch 41 finished \tANN training loss 0.035660\n",
      ">> Epoch 42 finished \tANN training loss 0.035345\n",
      ">> Epoch 43 finished \tANN training loss 0.035027\n",
      ">> Epoch 44 finished \tANN training loss 0.034715\n",
      ">> Epoch 45 finished \tANN training loss 0.034421\n",
      ">> Epoch 46 finished \tANN training loss 0.034118\n",
      ">> Epoch 47 finished \tANN training loss 0.033830\n",
      ">> Epoch 48 finished \tANN training loss 0.033538\n",
      ">> Epoch 49 finished \tANN training loss 0.033260\n",
      ">> Epoch 50 finished \tANN training loss 0.032977\n",
      ">> Epoch 51 finished \tANN training loss 0.032704\n",
      ">> Epoch 52 finished \tANN training loss 0.032428\n",
      ">> Epoch 53 finished \tANN training loss 0.032173\n",
      ">> Epoch 54 finished \tANN training loss 0.031928\n",
      ">> Epoch 55 finished \tANN training loss 0.031668\n",
      ">> Epoch 56 finished \tANN training loss 0.031425\n",
      ">> Epoch 57 finished \tANN training loss 0.031173\n",
      ">> Epoch 58 finished \tANN training loss 0.030945\n",
      ">> Epoch 59 finished \tANN training loss 0.030712\n",
      ">> Epoch 60 finished \tANN training loss 0.030475\n",
      ">> Epoch 61 finished \tANN training loss 0.030254\n",
      ">> Epoch 62 finished \tANN training loss 0.030036\n",
      ">> Epoch 63 finished \tANN training loss 0.029807\n",
      ">> Epoch 64 finished \tANN training loss 0.029605\n",
      ">> Epoch 65 finished \tANN training loss 0.029391\n",
      ">> Epoch 66 finished \tANN training loss 0.029192\n",
      ">> Epoch 67 finished \tANN training loss 0.028990\n",
      ">> Epoch 68 finished \tANN training loss 0.028790\n",
      ">> Epoch 69 finished \tANN training loss 0.028595\n",
      ">> Epoch 70 finished \tANN training loss 0.028406\n",
      ">> Epoch 71 finished \tANN training loss 0.028225\n",
      ">> Epoch 72 finished \tANN training loss 0.028038\n",
      ">> Epoch 73 finished \tANN training loss 0.027866\n",
      ">> Epoch 74 finished \tANN training loss 0.027680\n",
      ">> Epoch 75 finished \tANN training loss 0.027513\n",
      ">> Epoch 76 finished \tANN training loss 0.027345\n",
      ">> Epoch 77 finished \tANN training loss 0.027186\n",
      ">> Epoch 78 finished \tANN training loss 0.027019\n",
      ">> Epoch 79 finished \tANN training loss 0.026864\n",
      ">> Epoch 80 finished \tANN training loss 0.026703\n",
      ">> Epoch 81 finished \tANN training loss 0.026557\n",
      ">> Epoch 82 finished \tANN training loss 0.026410\n",
      ">> Epoch 83 finished \tANN training loss 0.026252\n",
      ">> Epoch 84 finished \tANN training loss 0.026110\n",
      ">> Epoch 85 finished \tANN training loss 0.025971\n",
      ">> Epoch 86 finished \tANN training loss 0.025834\n",
      ">> Epoch 87 finished \tANN training loss 0.025707\n",
      ">> Epoch 88 finished \tANN training loss 0.025573\n",
      ">> Epoch 89 finished \tANN training loss 0.025440\n",
      ">> Epoch 90 finished \tANN training loss 0.025315\n",
      ">> Epoch 91 finished \tANN training loss 0.025174\n",
      ">> Epoch 92 finished \tANN training loss 0.025061\n",
      ">> Epoch 93 finished \tANN training loss 0.024940\n",
      ">> Epoch 94 finished \tANN training loss 0.024828\n",
      ">> Epoch 95 finished \tANN training loss 0.024706\n",
      ">> Epoch 96 finished \tANN training loss 0.024592\n",
      ">> Epoch 97 finished \tANN training loss 0.024480\n",
      ">> Epoch 98 finished \tANN training loss 0.024372\n",
      ">> Epoch 99 finished \tANN training loss 0.024263\n",
      ">> Epoch 100 finished \tANN training loss 0.024162\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.374288\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.422287\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.438691\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.436365\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.426796\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.414381\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.400011\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.385361\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.369117\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.354447\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.460349\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.448396\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.446986\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.444747\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.442854\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.440821\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.438482\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.435919\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.433316\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.430730\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.091063\n",
      ">> Epoch 2 finished \tANN training loss 0.069523\n",
      ">> Epoch 3 finished \tANN training loss 0.068366\n",
      ">> Epoch 4 finished \tANN training loss 0.067215\n",
      ">> Epoch 5 finished \tANN training loss 0.066068\n",
      ">> Epoch 6 finished \tANN training loss 0.065011\n",
      ">> Epoch 7 finished \tANN training loss 0.063947\n",
      ">> Epoch 8 finished \tANN training loss 0.062930\n",
      ">> Epoch 9 finished \tANN training loss 0.061923\n",
      ">> Epoch 10 finished \tANN training loss 0.060941\n",
      ">> Epoch 11 finished \tANN training loss 0.059989\n",
      ">> Epoch 12 finished \tANN training loss 0.059066\n",
      ">> Epoch 13 finished \tANN training loss 0.058153\n",
      ">> Epoch 14 finished \tANN training loss 0.057270\n",
      ">> Epoch 15 finished \tANN training loss 0.056387\n",
      ">> Epoch 16 finished \tANN training loss 0.055544\n",
      ">> Epoch 17 finished \tANN training loss 0.054711\n",
      ">> Epoch 18 finished \tANN training loss 0.053891\n",
      ">> Epoch 19 finished \tANN training loss 0.053089\n",
      ">> Epoch 20 finished \tANN training loss 0.052322\n",
      ">> Epoch 21 finished \tANN training loss 0.051544\n",
      ">> Epoch 22 finished \tANN training loss 0.050822\n",
      ">> Epoch 23 finished \tANN training loss 0.050091\n",
      ">> Epoch 24 finished \tANN training loss 0.049370\n",
      ">> Epoch 25 finished \tANN training loss 0.048670\n",
      ">> Epoch 26 finished \tANN training loss 0.047975\n",
      ">> Epoch 27 finished \tANN training loss 0.047308\n",
      ">> Epoch 28 finished \tANN training loss 0.046636\n",
      ">> Epoch 29 finished \tANN training loss 0.045996\n",
      ">> Epoch 30 finished \tANN training loss 0.045362\n",
      ">> Epoch 31 finished \tANN training loss 0.044733\n",
      ">> Epoch 32 finished \tANN training loss 0.044131\n",
      ">> Epoch 33 finished \tANN training loss 0.043538\n",
      ">> Epoch 34 finished \tANN training loss 0.042930\n",
      ">> Epoch 35 finished \tANN training loss 0.042369\n",
      ">> Epoch 36 finished \tANN training loss 0.041800\n",
      ">> Epoch 37 finished \tANN training loss 0.041241\n",
      ">> Epoch 38 finished \tANN training loss 0.040719\n",
      ">> Epoch 39 finished \tANN training loss 0.040182\n",
      ">> Epoch 40 finished \tANN training loss 0.039661\n",
      ">> Epoch 41 finished \tANN training loss 0.039153\n",
      ">> Epoch 42 finished \tANN training loss 0.038641\n",
      ">> Epoch 43 finished \tANN training loss 0.038147\n",
      ">> Epoch 44 finished \tANN training loss 0.037682\n",
      ">> Epoch 45 finished \tANN training loss 0.037204\n",
      ">> Epoch 46 finished \tANN training loss 0.036747\n",
      ">> Epoch 47 finished \tANN training loss 0.036292\n",
      ">> Epoch 48 finished \tANN training loss 0.035839\n",
      ">> Epoch 49 finished \tANN training loss 0.035415\n",
      ">> Epoch 50 finished \tANN training loss 0.034988\n",
      ">> Epoch 51 finished \tANN training loss 0.034569\n",
      ">> Epoch 52 finished \tANN training loss 0.034162\n",
      ">> Epoch 53 finished \tANN training loss 0.033757\n",
      ">> Epoch 54 finished \tANN training loss 0.033363\n",
      ">> Epoch 55 finished \tANN training loss 0.032997\n",
      ">> Epoch 56 finished \tANN training loss 0.032619\n",
      ">> Epoch 57 finished \tANN training loss 0.032246\n",
      ">> Epoch 58 finished \tANN training loss 0.031894\n",
      ">> Epoch 59 finished \tANN training loss 0.031547\n",
      ">> Epoch 60 finished \tANN training loss 0.031195\n",
      ">> Epoch 61 finished \tANN training loss 0.030867\n",
      ">> Epoch 62 finished \tANN training loss 0.030533\n",
      ">> Epoch 63 finished \tANN training loss 0.030213\n",
      ">> Epoch 64 finished \tANN training loss 0.029896\n",
      ">> Epoch 65 finished \tANN training loss 0.029583\n",
      ">> Epoch 66 finished \tANN training loss 0.029280\n",
      ">> Epoch 67 finished \tANN training loss 0.028999\n",
      ">> Epoch 68 finished \tANN training loss 0.028711\n",
      ">> Epoch 69 finished \tANN training loss 0.028438\n",
      ">> Epoch 70 finished \tANN training loss 0.028161\n",
      ">> Epoch 71 finished \tANN training loss 0.027896\n",
      ">> Epoch 72 finished \tANN training loss 0.027627\n",
      ">> Epoch 73 finished \tANN training loss 0.027374\n",
      ">> Epoch 74 finished \tANN training loss 0.027131\n",
      ">> Epoch 75 finished \tANN training loss 0.026880\n",
      ">> Epoch 76 finished \tANN training loss 0.026643\n",
      ">> Epoch 77 finished \tANN training loss 0.026420\n",
      ">> Epoch 78 finished \tANN training loss 0.026185\n",
      ">> Epoch 79 finished \tANN training loss 0.025969\n",
      ">> Epoch 80 finished \tANN training loss 0.025754\n",
      ">> Epoch 81 finished \tANN training loss 0.025537\n",
      ">> Epoch 82 finished \tANN training loss 0.025334\n",
      ">> Epoch 83 finished \tANN training loss 0.025135\n",
      ">> Epoch 84 finished \tANN training loss 0.024931\n",
      ">> Epoch 85 finished \tANN training loss 0.024751\n",
      ">> Epoch 86 finished \tANN training loss 0.024559\n",
      ">> Epoch 87 finished \tANN training loss 0.024376\n",
      ">> Epoch 88 finished \tANN training loss 0.024197\n",
      ">> Epoch 89 finished \tANN training loss 0.024024\n",
      ">> Epoch 90 finished \tANN training loss 0.023858\n",
      ">> Epoch 91 finished \tANN training loss 0.023687\n",
      ">> Epoch 92 finished \tANN training loss 0.023534\n",
      ">> Epoch 93 finished \tANN training loss 0.023370\n",
      ">> Epoch 94 finished \tANN training loss 0.023215\n",
      ">> Epoch 95 finished \tANN training loss 0.023073\n",
      ">> Epoch 96 finished \tANN training loss 0.022918\n",
      ">> Epoch 97 finished \tANN training loss 0.022775\n",
      ">> Epoch 98 finished \tANN training loss 0.022625\n",
      ">> Epoch 99 finished \tANN training loss 0.022501\n",
      ">> Epoch 100 finished \tANN training loss 0.022354\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.354553\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.410343\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.430905\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.430777\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.421486\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.408306\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.393936\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.379581\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.362964\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.348695\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.470381\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.458911\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.456983\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.454663\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.452274\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.449781\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.447040\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.444178\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.441308\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.438276\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.071442\n",
      ">> Epoch 2 finished \tANN training loss 0.060552\n",
      ">> Epoch 3 finished \tANN training loss 0.059833\n",
      ">> Epoch 4 finished \tANN training loss 0.059118\n",
      ">> Epoch 5 finished \tANN training loss 0.058430\n",
      ">> Epoch 6 finished \tANN training loss 0.057737\n",
      ">> Epoch 7 finished \tANN training loss 0.057063\n",
      ">> Epoch 8 finished \tANN training loss 0.056420\n",
      ">> Epoch 9 finished \tANN training loss 0.055768\n",
      ">> Epoch 10 finished \tANN training loss 0.055126\n",
      ">> Epoch 11 finished \tANN training loss 0.054501\n",
      ">> Epoch 12 finished \tANN training loss 0.053885\n",
      ">> Epoch 13 finished \tANN training loss 0.053283\n",
      ">> Epoch 14 finished \tANN training loss 0.052659\n",
      ">> Epoch 15 finished \tANN training loss 0.052097\n",
      ">> Epoch 16 finished \tANN training loss 0.051501\n",
      ">> Epoch 17 finished \tANN training loss 0.050936\n",
      ">> Epoch 18 finished \tANN training loss 0.050376\n",
      ">> Epoch 19 finished \tANN training loss 0.049830\n",
      ">> Epoch 20 finished \tANN training loss 0.049273\n",
      ">> Epoch 21 finished \tANN training loss 0.048739\n",
      ">> Epoch 22 finished \tANN training loss 0.048200\n",
      ">> Epoch 23 finished \tANN training loss 0.047690\n",
      ">> Epoch 24 finished \tANN training loss 0.047172\n",
      ">> Epoch 25 finished \tANN training loss 0.046655\n",
      ">> Epoch 26 finished \tANN training loss 0.046159\n",
      ">> Epoch 27 finished \tANN training loss 0.045660\n",
      ">> Epoch 28 finished \tANN training loss 0.045171\n",
      ">> Epoch 29 finished \tANN training loss 0.044685\n",
      ">> Epoch 30 finished \tANN training loss 0.044217\n",
      ">> Epoch 31 finished \tANN training loss 0.043744\n",
      ">> Epoch 32 finished \tANN training loss 0.043275\n",
      ">> Epoch 33 finished \tANN training loss 0.042827\n",
      ">> Epoch 34 finished \tANN training loss 0.042372\n",
      ">> Epoch 35 finished \tANN training loss 0.041932\n",
      ">> Epoch 36 finished \tANN training loss 0.041482\n",
      ">> Epoch 37 finished \tANN training loss 0.041047\n",
      ">> Epoch 38 finished \tANN training loss 0.040631\n",
      ">> Epoch 39 finished \tANN training loss 0.040213\n",
      ">> Epoch 40 finished \tANN training loss 0.039798\n",
      ">> Epoch 41 finished \tANN training loss 0.039387\n",
      ">> Epoch 42 finished \tANN training loss 0.038986\n",
      ">> Epoch 43 finished \tANN training loss 0.038585\n",
      ">> Epoch 44 finished \tANN training loss 0.038189\n",
      ">> Epoch 45 finished \tANN training loss 0.037815\n",
      ">> Epoch 46 finished \tANN training loss 0.037428\n",
      ">> Epoch 47 finished \tANN training loss 0.037047\n",
      ">> Epoch 48 finished \tANN training loss 0.036673\n",
      ">> Epoch 49 finished \tANN training loss 0.036320\n",
      ">> Epoch 50 finished \tANN training loss 0.035952\n",
      ">> Epoch 51 finished \tANN training loss 0.035597\n",
      ">> Epoch 52 finished \tANN training loss 0.035245\n",
      ">> Epoch 53 finished \tANN training loss 0.034907\n",
      ">> Epoch 54 finished \tANN training loss 0.034567\n",
      ">> Epoch 55 finished \tANN training loss 0.034234\n",
      ">> Epoch 56 finished \tANN training loss 0.033896\n",
      ">> Epoch 57 finished \tANN training loss 0.033583\n",
      ">> Epoch 58 finished \tANN training loss 0.033255\n",
      ">> Epoch 59 finished \tANN training loss 0.032944\n",
      ">> Epoch 60 finished \tANN training loss 0.032636\n",
      ">> Epoch 61 finished \tANN training loss 0.032329\n",
      ">> Epoch 62 finished \tANN training loss 0.032020\n",
      ">> Epoch 63 finished \tANN training loss 0.031736\n",
      ">> Epoch 64 finished \tANN training loss 0.031445\n",
      ">> Epoch 65 finished \tANN training loss 0.031159\n",
      ">> Epoch 66 finished \tANN training loss 0.030875\n",
      ">> Epoch 67 finished \tANN training loss 0.030592\n",
      ">> Epoch 68 finished \tANN training loss 0.030324\n",
      ">> Epoch 69 finished \tANN training loss 0.030066\n",
      ">> Epoch 70 finished \tANN training loss 0.029796\n",
      ">> Epoch 71 finished \tANN training loss 0.029545\n",
      ">> Epoch 72 finished \tANN training loss 0.029281\n",
      ">> Epoch 73 finished \tANN training loss 0.029040\n",
      ">> Epoch 74 finished \tANN training loss 0.028786\n",
      ">> Epoch 75 finished \tANN training loss 0.028556\n",
      ">> Epoch 76 finished \tANN training loss 0.028316\n",
      ">> Epoch 77 finished \tANN training loss 0.028084\n",
      ">> Epoch 78 finished \tANN training loss 0.027852\n",
      ">> Epoch 79 finished \tANN training loss 0.027618\n",
      ">> Epoch 80 finished \tANN training loss 0.027403\n",
      ">> Epoch 81 finished \tANN training loss 0.027190\n",
      ">> Epoch 82 finished \tANN training loss 0.026974\n",
      ">> Epoch 83 finished \tANN training loss 0.026767\n",
      ">> Epoch 84 finished \tANN training loss 0.026557\n",
      ">> Epoch 85 finished \tANN training loss 0.026358\n",
      ">> Epoch 86 finished \tANN training loss 0.026160\n",
      ">> Epoch 87 finished \tANN training loss 0.025965\n",
      ">> Epoch 88 finished \tANN training loss 0.025775\n",
      ">> Epoch 89 finished \tANN training loss 0.025589\n",
      ">> Epoch 90 finished \tANN training loss 0.025404\n",
      ">> Epoch 91 finished \tANN training loss 0.025220\n",
      ">> Epoch 92 finished \tANN training loss 0.025045\n",
      ">> Epoch 93 finished \tANN training loss 0.024872\n",
      ">> Epoch 94 finished \tANN training loss 0.024691\n",
      ">> Epoch 95 finished \tANN training loss 0.024529\n",
      ">> Epoch 96 finished \tANN training loss 0.024364\n",
      ">> Epoch 97 finished \tANN training loss 0.024196\n",
      ">> Epoch 98 finished \tANN training loss 0.024047\n",
      ">> Epoch 99 finished \tANN training loss 0.023891\n",
      ">> Epoch 100 finished \tANN training loss 0.023736\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.360187\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.415841\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.435300\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.434209\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.424952\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.409285\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.395858\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.378729\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.363156\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.346787\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.470190\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.457480\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.455294\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.452778\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.450356\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.447252\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.444397\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.441493\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.438483\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.435189\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.066936\n",
      ">> Epoch 2 finished \tANN training loss 0.065858\n",
      ">> Epoch 3 finished \tANN training loss 0.064789\n",
      ">> Epoch 4 finished \tANN training loss 0.063768\n",
      ">> Epoch 5 finished \tANN training loss 0.062750\n",
      ">> Epoch 6 finished \tANN training loss 0.061773\n",
      ">> Epoch 7 finished \tANN training loss 0.060818\n",
      ">> Epoch 8 finished \tANN training loss 0.059880\n",
      ">> Epoch 9 finished \tANN training loss 0.058982\n",
      ">> Epoch 10 finished \tANN training loss 0.058095\n",
      ">> Epoch 11 finished \tANN training loss 0.057229\n",
      ">> Epoch 12 finished \tANN training loss 0.056361\n",
      ">> Epoch 13 finished \tANN training loss 0.055541\n",
      ">> Epoch 14 finished \tANN training loss 0.054725\n",
      ">> Epoch 15 finished \tANN training loss 0.053934\n",
      ">> Epoch 16 finished \tANN training loss 0.053163\n",
      ">> Epoch 17 finished \tANN training loss 0.052407\n",
      ">> Epoch 18 finished \tANN training loss 0.051656\n",
      ">> Epoch 19 finished \tANN training loss 0.050931\n",
      ">> Epoch 20 finished \tANN training loss 0.050212\n",
      ">> Epoch 21 finished \tANN training loss 0.049518\n",
      ">> Epoch 22 finished \tANN training loss 0.048828\n",
      ">> Epoch 23 finished \tANN training loss 0.048164\n",
      ">> Epoch 24 finished \tANN training loss 0.047509\n",
      ">> Epoch 25 finished \tANN training loss 0.046864\n",
      ">> Epoch 26 finished \tANN training loss 0.046238\n",
      ">> Epoch 27 finished \tANN training loss 0.045620\n",
      ">> Epoch 28 finished \tANN training loss 0.045010\n",
      ">> Epoch 29 finished \tANN training loss 0.044431\n",
      ">> Epoch 30 finished \tANN training loss 0.043839\n",
      ">> Epoch 31 finished \tANN training loss 0.043277\n",
      ">> Epoch 32 finished \tANN training loss 0.042720\n",
      ">> Epoch 33 finished \tANN training loss 0.042168\n",
      ">> Epoch 34 finished \tANN training loss 0.041629\n",
      ">> Epoch 35 finished \tANN training loss 0.041110\n",
      ">> Epoch 36 finished \tANN training loss 0.040597\n",
      ">> Epoch 37 finished \tANN training loss 0.040082\n",
      ">> Epoch 38 finished \tANN training loss 0.039597\n",
      ">> Epoch 39 finished \tANN training loss 0.039113\n",
      ">> Epoch 40 finished \tANN training loss 0.038637\n",
      ">> Epoch 41 finished \tANN training loss 0.038177\n",
      ">> Epoch 42 finished \tANN training loss 0.037707\n",
      ">> Epoch 43 finished \tANN training loss 0.037278\n",
      ">> Epoch 44 finished \tANN training loss 0.036827\n",
      ">> Epoch 45 finished \tANN training loss 0.036424\n",
      ">> Epoch 46 finished \tANN training loss 0.035998\n",
      ">> Epoch 47 finished \tANN training loss 0.035592\n",
      ">> Epoch 48 finished \tANN training loss 0.035182\n",
      ">> Epoch 49 finished \tANN training loss 0.034802\n",
      ">> Epoch 50 finished \tANN training loss 0.034424\n",
      ">> Epoch 51 finished \tANN training loss 0.034037\n",
      ">> Epoch 52 finished \tANN training loss 0.033677\n",
      ">> Epoch 53 finished \tANN training loss 0.033325\n",
      ">> Epoch 54 finished \tANN training loss 0.032974\n",
      ">> Epoch 55 finished \tANN training loss 0.032629\n",
      ">> Epoch 56 finished \tANN training loss 0.032292\n",
      ">> Epoch 57 finished \tANN training loss 0.031961\n",
      ">> Epoch 58 finished \tANN training loss 0.031637\n",
      ">> Epoch 59 finished \tANN training loss 0.031329\n",
      ">> Epoch 60 finished \tANN training loss 0.031016\n",
      ">> Epoch 61 finished \tANN training loss 0.030725\n",
      ">> Epoch 62 finished \tANN training loss 0.030431\n",
      ">> Epoch 63 finished \tANN training loss 0.030141\n",
      ">> Epoch 64 finished \tANN training loss 0.029864\n",
      ">> Epoch 65 finished \tANN training loss 0.029590\n",
      ">> Epoch 66 finished \tANN training loss 0.029325\n",
      ">> Epoch 67 finished \tANN training loss 0.029067\n",
      ">> Epoch 68 finished \tANN training loss 0.028808\n",
      ">> Epoch 69 finished \tANN training loss 0.028560\n",
      ">> Epoch 70 finished \tANN training loss 0.028326\n",
      ">> Epoch 71 finished \tANN training loss 0.028085\n",
      ">> Epoch 72 finished \tANN training loss 0.027844\n",
      ">> Epoch 73 finished \tANN training loss 0.027627\n",
      ">> Epoch 74 finished \tANN training loss 0.027402\n",
      ">> Epoch 75 finished \tANN training loss 0.027183\n",
      ">> Epoch 76 finished \tANN training loss 0.026982\n",
      ">> Epoch 77 finished \tANN training loss 0.026760\n",
      ">> Epoch 78 finished \tANN training loss 0.026574\n",
      ">> Epoch 79 finished \tANN training loss 0.026378\n",
      ">> Epoch 80 finished \tANN training loss 0.026188\n",
      ">> Epoch 81 finished \tANN training loss 0.026000\n",
      ">> Epoch 82 finished \tANN training loss 0.025820\n",
      ">> Epoch 83 finished \tANN training loss 0.025640\n",
      ">> Epoch 84 finished \tANN training loss 0.025470\n",
      ">> Epoch 85 finished \tANN training loss 0.025298\n",
      ">> Epoch 86 finished \tANN training loss 0.025130\n",
      ">> Epoch 87 finished \tANN training loss 0.024966\n",
      ">> Epoch 88 finished \tANN training loss 0.024812\n",
      ">> Epoch 89 finished \tANN training loss 0.024659\n",
      ">> Epoch 90 finished \tANN training loss 0.024509\n",
      ">> Epoch 91 finished \tANN training loss 0.024361\n",
      ">> Epoch 92 finished \tANN training loss 0.024211\n",
      ">> Epoch 93 finished \tANN training loss 0.024073\n",
      ">> Epoch 94 finished \tANN training loss 0.023939\n",
      ">> Epoch 95 finished \tANN training loss 0.023807\n",
      ">> Epoch 96 finished \tANN training loss 0.023672\n",
      ">> Epoch 97 finished \tANN training loss 0.023544\n",
      ">> Epoch 98 finished \tANN training loss 0.023423\n",
      ">> Epoch 99 finished \tANN training loss 0.023299\n",
      ">> Epoch 100 finished \tANN training loss 0.023178\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.322613\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.380596\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.401920\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.398028\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.389791\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.375361\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.360951\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.343490\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.328953\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.313850\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.502408\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.490791\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.488602\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.485791\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.482932\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.479770\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.476264\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.473133\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.469767\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.466218\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.057693\n",
      ">> Epoch 2 finished \tANN training loss 0.056392\n",
      ">> Epoch 3 finished \tANN training loss 0.055706\n",
      ">> Epoch 4 finished \tANN training loss 0.055003\n",
      ">> Epoch 5 finished \tANN training loss 0.054319\n",
      ">> Epoch 6 finished \tANN training loss 0.053642\n",
      ">> Epoch 7 finished \tANN training loss 0.052994\n",
      ">> Epoch 8 finished \tANN training loss 0.052357\n",
      ">> Epoch 9 finished \tANN training loss 0.051737\n",
      ">> Epoch 10 finished \tANN training loss 0.051108\n",
      ">> Epoch 11 finished \tANN training loss 0.050504\n",
      ">> Epoch 12 finished \tANN training loss 0.049915\n",
      ">> Epoch 13 finished \tANN training loss 0.049333\n",
      ">> Epoch 14 finished \tANN training loss 0.048755\n",
      ">> Epoch 15 finished \tANN training loss 0.048194\n",
      ">> Epoch 16 finished \tANN training loss 0.047637\n",
      ">> Epoch 17 finished \tANN training loss 0.047088\n",
      ">> Epoch 18 finished \tANN training loss 0.046565\n",
      ">> Epoch 19 finished \tANN training loss 0.046038\n",
      ">> Epoch 20 finished \tANN training loss 0.045534\n",
      ">> Epoch 21 finished \tANN training loss 0.045032\n",
      ">> Epoch 22 finished \tANN training loss 0.044531\n",
      ">> Epoch 23 finished \tANN training loss 0.044047\n",
      ">> Epoch 24 finished \tANN training loss 0.043550\n",
      ">> Epoch 25 finished \tANN training loss 0.043100\n",
      ">> Epoch 26 finished \tANN training loss 0.042631\n",
      ">> Epoch 27 finished \tANN training loss 0.042181\n",
      ">> Epoch 28 finished \tANN training loss 0.041735\n",
      ">> Epoch 29 finished \tANN training loss 0.041306\n",
      ">> Epoch 30 finished \tANN training loss 0.040866\n",
      ">> Epoch 31 finished \tANN training loss 0.040449\n",
      ">> Epoch 32 finished \tANN training loss 0.040036\n",
      ">> Epoch 33 finished \tANN training loss 0.039625\n",
      ">> Epoch 34 finished \tANN training loss 0.039215\n",
      ">> Epoch 35 finished \tANN training loss 0.038822\n",
      ">> Epoch 36 finished \tANN training loss 0.038453\n",
      ">> Epoch 37 finished \tANN training loss 0.038065\n",
      ">> Epoch 38 finished \tANN training loss 0.037688\n",
      ">> Epoch 39 finished \tANN training loss 0.037327\n",
      ">> Epoch 40 finished \tANN training loss 0.036959\n",
      ">> Epoch 41 finished \tANN training loss 0.036598\n",
      ">> Epoch 42 finished \tANN training loss 0.036270\n",
      ">> Epoch 43 finished \tANN training loss 0.035931\n",
      ">> Epoch 44 finished \tANN training loss 0.035605\n",
      ">> Epoch 45 finished \tANN training loss 0.035269\n",
      ">> Epoch 46 finished \tANN training loss 0.034953\n",
      ">> Epoch 47 finished \tANN training loss 0.034640\n",
      ">> Epoch 48 finished \tANN training loss 0.034327\n",
      ">> Epoch 49 finished \tANN training loss 0.034022\n",
      ">> Epoch 50 finished \tANN training loss 0.033723\n",
      ">> Epoch 51 finished \tANN training loss 0.033430\n",
      ">> Epoch 52 finished \tANN training loss 0.033140\n",
      ">> Epoch 53 finished \tANN training loss 0.032869\n",
      ">> Epoch 54 finished \tANN training loss 0.032582\n",
      ">> Epoch 55 finished \tANN training loss 0.032320\n",
      ">> Epoch 56 finished \tANN training loss 0.032058\n",
      ">> Epoch 57 finished \tANN training loss 0.031786\n",
      ">> Epoch 58 finished \tANN training loss 0.031537\n",
      ">> Epoch 59 finished \tANN training loss 0.031288\n",
      ">> Epoch 60 finished \tANN training loss 0.031045\n",
      ">> Epoch 61 finished \tANN training loss 0.030800\n",
      ">> Epoch 62 finished \tANN training loss 0.030569\n",
      ">> Epoch 63 finished \tANN training loss 0.030338\n",
      ">> Epoch 64 finished \tANN training loss 0.030115\n",
      ">> Epoch 65 finished \tANN training loss 0.029888\n",
      ">> Epoch 66 finished \tANN training loss 0.029673\n",
      ">> Epoch 67 finished \tANN training loss 0.029460\n",
      ">> Epoch 68 finished \tANN training loss 0.029248\n",
      ">> Epoch 69 finished \tANN training loss 0.029047\n",
      ">> Epoch 70 finished \tANN training loss 0.028847\n",
      ">> Epoch 71 finished \tANN training loss 0.028650\n",
      ">> Epoch 72 finished \tANN training loss 0.028456\n",
      ">> Epoch 73 finished \tANN training loss 0.028271\n",
      ">> Epoch 74 finished \tANN training loss 0.028081\n",
      ">> Epoch 75 finished \tANN training loss 0.027911\n",
      ">> Epoch 76 finished \tANN training loss 0.027731\n",
      ">> Epoch 77 finished \tANN training loss 0.027551\n",
      ">> Epoch 78 finished \tANN training loss 0.027386\n",
      ">> Epoch 79 finished \tANN training loss 0.027221\n",
      ">> Epoch 80 finished \tANN training loss 0.027060\n",
      ">> Epoch 81 finished \tANN training loss 0.026892\n",
      ">> Epoch 82 finished \tANN training loss 0.026734\n",
      ">> Epoch 83 finished \tANN training loss 0.026575\n",
      ">> Epoch 84 finished \tANN training loss 0.026434\n",
      ">> Epoch 85 finished \tANN training loss 0.026281\n",
      ">> Epoch 86 finished \tANN training loss 0.026137\n",
      ">> Epoch 87 finished \tANN training loss 0.025994\n",
      ">> Epoch 88 finished \tANN training loss 0.025859\n",
      ">> Epoch 89 finished \tANN training loss 0.025723\n",
      ">> Epoch 90 finished \tANN training loss 0.025588\n",
      ">> Epoch 91 finished \tANN training loss 0.025456\n",
      ">> Epoch 92 finished \tANN training loss 0.025324\n",
      ">> Epoch 93 finished \tANN training loss 0.025199\n",
      ">> Epoch 94 finished \tANN training loss 0.025073\n",
      ">> Epoch 95 finished \tANN training loss 0.024951\n",
      ">> Epoch 96 finished \tANN training loss 0.024830\n",
      ">> Epoch 97 finished \tANN training loss 0.024715\n",
      ">> Epoch 98 finished \tANN training loss 0.024599\n",
      ">> Epoch 99 finished \tANN training loss 0.024479\n",
      ">> Epoch 100 finished \tANN training loss 0.024377\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.346882\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.403806\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.422513\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.420916\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.408044\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.392846\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.377414\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.361507\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.345754\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.327361\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.494130\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.484655\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.481844\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.479012\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.475915\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.472701\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.469212\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.465491\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.462103\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.458492\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.076278\n",
      ">> Epoch 2 finished \tANN training loss 0.064658\n",
      ">> Epoch 3 finished \tANN training loss 0.063605\n",
      ">> Epoch 4 finished \tANN training loss 0.062585\n",
      ">> Epoch 5 finished \tANN training loss 0.061578\n",
      ">> Epoch 6 finished \tANN training loss 0.060587\n",
      ">> Epoch 7 finished \tANN training loss 0.059645\n",
      ">> Epoch 8 finished \tANN training loss 0.058691\n",
      ">> Epoch 9 finished \tANN training loss 0.057813\n",
      ">> Epoch 10 finished \tANN training loss 0.056896\n",
      ">> Epoch 11 finished \tANN training loss 0.056044\n",
      ">> Epoch 12 finished \tANN training loss 0.055210\n",
      ">> Epoch 13 finished \tANN training loss 0.054374\n",
      ">> Epoch 14 finished \tANN training loss 0.053562\n",
      ">> Epoch 15 finished \tANN training loss 0.052770\n",
      ">> Epoch 16 finished \tANN training loss 0.051999\n",
      ">> Epoch 17 finished \tANN training loss 0.051233\n",
      ">> Epoch 18 finished \tANN training loss 0.050486\n",
      ">> Epoch 19 finished \tANN training loss 0.049764\n",
      ">> Epoch 20 finished \tANN training loss 0.049049\n",
      ">> Epoch 21 finished \tANN training loss 0.048361\n",
      ">> Epoch 22 finished \tANN training loss 0.047670\n",
      ">> Epoch 23 finished \tANN training loss 0.046995\n",
      ">> Epoch 24 finished \tANN training loss 0.046348\n",
      ">> Epoch 25 finished \tANN training loss 0.045705\n",
      ">> Epoch 26 finished \tANN training loss 0.045063\n",
      ">> Epoch 27 finished \tANN training loss 0.044462\n",
      ">> Epoch 28 finished \tANN training loss 0.043839\n",
      ">> Epoch 29 finished \tANN training loss 0.043266\n",
      ">> Epoch 30 finished \tANN training loss 0.042678\n",
      ">> Epoch 31 finished \tANN training loss 0.042112\n",
      ">> Epoch 32 finished \tANN training loss 0.041545\n",
      ">> Epoch 33 finished \tANN training loss 0.041005\n",
      ">> Epoch 34 finished \tANN training loss 0.040472\n",
      ">> Epoch 35 finished \tANN training loss 0.039951\n",
      ">> Epoch 36 finished \tANN training loss 0.039429\n",
      ">> Epoch 37 finished \tANN training loss 0.038927\n",
      ">> Epoch 38 finished \tANN training loss 0.038432\n",
      ">> Epoch 39 finished \tANN training loss 0.037954\n",
      ">> Epoch 40 finished \tANN training loss 0.037485\n",
      ">> Epoch 41 finished \tANN training loss 0.037011\n",
      ">> Epoch 42 finished \tANN training loss 0.036561\n",
      ">> Epoch 43 finished \tANN training loss 0.036118\n",
      ">> Epoch 44 finished \tANN training loss 0.035681\n",
      ">> Epoch 45 finished \tANN training loss 0.035252\n",
      ">> Epoch 46 finished \tANN training loss 0.034829\n",
      ">> Epoch 47 finished \tANN training loss 0.034422\n",
      ">> Epoch 48 finished \tANN training loss 0.034031\n",
      ">> Epoch 49 finished \tANN training loss 0.033625\n",
      ">> Epoch 50 finished \tANN training loss 0.033256\n",
      ">> Epoch 51 finished \tANN training loss 0.032876\n",
      ">> Epoch 52 finished \tANN training loss 0.032514\n",
      ">> Epoch 53 finished \tANN training loss 0.032151\n",
      ">> Epoch 54 finished \tANN training loss 0.031810\n",
      ">> Epoch 55 finished \tANN training loss 0.031456\n",
      ">> Epoch 56 finished \tANN training loss 0.031126\n",
      ">> Epoch 57 finished \tANN training loss 0.030794\n",
      ">> Epoch 58 finished \tANN training loss 0.030477\n",
      ">> Epoch 59 finished \tANN training loss 0.030165\n",
      ">> Epoch 60 finished \tANN training loss 0.029855\n",
      ">> Epoch 61 finished \tANN training loss 0.029552\n",
      ">> Epoch 62 finished \tANN training loss 0.029265\n",
      ">> Epoch 63 finished \tANN training loss 0.028979\n",
      ">> Epoch 64 finished \tANN training loss 0.028693\n",
      ">> Epoch 65 finished \tANN training loss 0.028428\n",
      ">> Epoch 66 finished \tANN training loss 0.028162\n",
      ">> Epoch 67 finished \tANN training loss 0.027900\n",
      ">> Epoch 68 finished \tANN training loss 0.027650\n",
      ">> Epoch 69 finished \tANN training loss 0.027382\n",
      ">> Epoch 70 finished \tANN training loss 0.027146\n",
      ">> Epoch 71 finished \tANN training loss 0.026927\n",
      ">> Epoch 72 finished \tANN training loss 0.026691\n",
      ">> Epoch 73 finished \tANN training loss 0.026459\n",
      ">> Epoch 74 finished \tANN training loss 0.026243\n",
      ">> Epoch 75 finished \tANN training loss 0.026026\n",
      ">> Epoch 76 finished \tANN training loss 0.025817\n",
      ">> Epoch 77 finished \tANN training loss 0.025615\n",
      ">> Epoch 78 finished \tANN training loss 0.025407\n",
      ">> Epoch 79 finished \tANN training loss 0.025217\n",
      ">> Epoch 80 finished \tANN training loss 0.025031\n",
      ">> Epoch 81 finished \tANN training loss 0.024845\n",
      ">> Epoch 82 finished \tANN training loss 0.024660\n",
      ">> Epoch 83 finished \tANN training loss 0.024477\n",
      ">> Epoch 84 finished \tANN training loss 0.024318\n",
      ">> Epoch 85 finished \tANN training loss 0.024142\n",
      ">> Epoch 86 finished \tANN training loss 0.023971\n",
      ">> Epoch 87 finished \tANN training loss 0.023823\n",
      ">> Epoch 88 finished \tANN training loss 0.023659\n",
      ">> Epoch 89 finished \tANN training loss 0.023511\n",
      ">> Epoch 90 finished \tANN training loss 0.023365\n",
      ">> Epoch 91 finished \tANN training loss 0.023216\n",
      ">> Epoch 92 finished \tANN training loss 0.023076\n",
      ">> Epoch 93 finished \tANN training loss 0.022931\n",
      ">> Epoch 94 finished \tANN training loss 0.022799\n",
      ">> Epoch 95 finished \tANN training loss 0.022667\n",
      ">> Epoch 96 finished \tANN training loss 0.022540\n",
      ">> Epoch 97 finished \tANN training loss 0.022409\n",
      ">> Epoch 98 finished \tANN training loss 0.022289\n",
      ">> Epoch 99 finished \tANN training loss 0.022167\n",
      ">> Epoch 100 finished \tANN training loss 0.022047\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.358941\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.410067\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.432715\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.429813\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.422474\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.409544\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.394504\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.378809\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.363324\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.347355\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.459014\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.448205\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.445911\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.443559\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.441068\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.438449\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.435168\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.432459\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.429235\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.426405\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.063288\n",
      ">> Epoch 2 finished \tANN training loss 0.058461\n",
      ">> Epoch 3 finished \tANN training loss 0.057768\n",
      ">> Epoch 4 finished \tANN training loss 0.057082\n",
      ">> Epoch 5 finished \tANN training loss 0.056437\n",
      ">> Epoch 6 finished \tANN training loss 0.055798\n",
      ">> Epoch 7 finished \tANN training loss 0.055159\n",
      ">> Epoch 8 finished \tANN training loss 0.054510\n",
      ">> Epoch 9 finished \tANN training loss 0.053900\n",
      ">> Epoch 10 finished \tANN training loss 0.053308\n",
      ">> Epoch 11 finished \tANN training loss 0.052695\n",
      ">> Epoch 12 finished \tANN training loss 0.052129\n",
      ">> Epoch 13 finished \tANN training loss 0.051552\n",
      ">> Epoch 14 finished \tANN training loss 0.050986\n",
      ">> Epoch 15 finished \tANN training loss 0.050421\n",
      ">> Epoch 16 finished \tANN training loss 0.049872\n",
      ">> Epoch 17 finished \tANN training loss 0.049331\n",
      ">> Epoch 18 finished \tANN training loss 0.048803\n",
      ">> Epoch 19 finished \tANN training loss 0.048278\n",
      ">> Epoch 20 finished \tANN training loss 0.047765\n",
      ">> Epoch 21 finished \tANN training loss 0.047255\n",
      ">> Epoch 22 finished \tANN training loss 0.046760\n",
      ">> Epoch 23 finished \tANN training loss 0.046268\n",
      ">> Epoch 24 finished \tANN training loss 0.045776\n",
      ">> Epoch 25 finished \tANN training loss 0.045300\n",
      ">> Epoch 26 finished \tANN training loss 0.044835\n",
      ">> Epoch 27 finished \tANN training loss 0.044363\n",
      ">> Epoch 28 finished \tANN training loss 0.043898\n",
      ">> Epoch 29 finished \tANN training loss 0.043459\n",
      ">> Epoch 30 finished \tANN training loss 0.043016\n",
      ">> Epoch 31 finished \tANN training loss 0.042586\n",
      ">> Epoch 32 finished \tANN training loss 0.042155\n",
      ">> Epoch 33 finished \tANN training loss 0.041734\n",
      ">> Epoch 34 finished \tANN training loss 0.041318\n",
      ">> Epoch 35 finished \tANN training loss 0.040909\n",
      ">> Epoch 36 finished \tANN training loss 0.040489\n",
      ">> Epoch 37 finished \tANN training loss 0.040092\n",
      ">> Epoch 38 finished \tANN training loss 0.039720\n",
      ">> Epoch 39 finished \tANN training loss 0.039329\n",
      ">> Epoch 40 finished \tANN training loss 0.038947\n",
      ">> Epoch 41 finished \tANN training loss 0.038577\n",
      ">> Epoch 42 finished \tANN training loss 0.038205\n",
      ">> Epoch 43 finished \tANN training loss 0.037829\n",
      ">> Epoch 44 finished \tANN training loss 0.037493\n",
      ">> Epoch 45 finished \tANN training loss 0.037142\n",
      ">> Epoch 46 finished \tANN training loss 0.036800\n",
      ">> Epoch 47 finished \tANN training loss 0.036466\n",
      ">> Epoch 48 finished \tANN training loss 0.036125\n",
      ">> Epoch 49 finished \tANN training loss 0.035802\n",
      ">> Epoch 50 finished \tANN training loss 0.035476\n",
      ">> Epoch 51 finished \tANN training loss 0.035163\n",
      ">> Epoch 52 finished \tANN training loss 0.034840\n",
      ">> Epoch 53 finished \tANN training loss 0.034544\n",
      ">> Epoch 54 finished \tANN training loss 0.034244\n",
      ">> Epoch 55 finished \tANN training loss 0.033946\n",
      ">> Epoch 56 finished \tANN training loss 0.033651\n",
      ">> Epoch 57 finished \tANN training loss 0.033361\n",
      ">> Epoch 58 finished \tANN training loss 0.033093\n",
      ">> Epoch 59 finished \tANN training loss 0.032813\n",
      ">> Epoch 60 finished \tANN training loss 0.032536\n",
      ">> Epoch 61 finished \tANN training loss 0.032272\n",
      ">> Epoch 62 finished \tANN training loss 0.032014\n",
      ">> Epoch 63 finished \tANN training loss 0.031745\n",
      ">> Epoch 64 finished \tANN training loss 0.031502\n",
      ">> Epoch 65 finished \tANN training loss 0.031244\n",
      ">> Epoch 66 finished \tANN training loss 0.031011\n",
      ">> Epoch 67 finished \tANN training loss 0.030772\n",
      ">> Epoch 68 finished \tANN training loss 0.030530\n",
      ">> Epoch 69 finished \tANN training loss 0.030300\n",
      ">> Epoch 70 finished \tANN training loss 0.030072\n",
      ">> Epoch 71 finished \tANN training loss 0.029846\n",
      ">> Epoch 72 finished \tANN training loss 0.029633\n",
      ">> Epoch 73 finished \tANN training loss 0.029412\n",
      ">> Epoch 74 finished \tANN training loss 0.029207\n",
      ">> Epoch 75 finished \tANN training loss 0.029009\n",
      ">> Epoch 76 finished \tANN training loss 0.028793\n",
      ">> Epoch 77 finished \tANN training loss 0.028604\n",
      ">> Epoch 78 finished \tANN training loss 0.028407\n",
      ">> Epoch 79 finished \tANN training loss 0.028201\n",
      ">> Epoch 80 finished \tANN training loss 0.028027\n",
      ">> Epoch 81 finished \tANN training loss 0.027841\n",
      ">> Epoch 82 finished \tANN training loss 0.027662\n",
      ">> Epoch 83 finished \tANN training loss 0.027479\n",
      ">> Epoch 84 finished \tANN training loss 0.027294\n",
      ">> Epoch 85 finished \tANN training loss 0.027145\n",
      ">> Epoch 86 finished \tANN training loss 0.026965\n",
      ">> Epoch 87 finished \tANN training loss 0.026798\n",
      ">> Epoch 88 finished \tANN training loss 0.026644\n",
      ">> Epoch 89 finished \tANN training loss 0.026482\n",
      ">> Epoch 90 finished \tANN training loss 0.026326\n",
      ">> Epoch 91 finished \tANN training loss 0.026169\n",
      ">> Epoch 92 finished \tANN training loss 0.026017\n",
      ">> Epoch 93 finished \tANN training loss 0.025874\n",
      ">> Epoch 94 finished \tANN training loss 0.025723\n",
      ">> Epoch 95 finished \tANN training loss 0.025587\n",
      ">> Epoch 96 finished \tANN training loss 0.025445\n",
      ">> Epoch 97 finished \tANN training loss 0.025308\n",
      ">> Epoch 98 finished \tANN training loss 0.025177\n",
      ">> Epoch 99 finished \tANN training loss 0.025039\n",
      ">> Epoch 100 finished \tANN training loss 0.024915\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.441793\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.476373\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.498263\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.495340\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.487687\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.476308\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.461251\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.447165\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.431418\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.414704\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.383888\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.375552\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.374474\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.372998\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.370822\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.368939\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.367054\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.365052\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.363058\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.360706\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.057997\n",
      ">> Epoch 2 finished \tANN training loss 0.051878\n",
      ">> Epoch 3 finished \tANN training loss 0.051257\n",
      ">> Epoch 4 finished \tANN training loss 0.050659\n",
      ">> Epoch 5 finished \tANN training loss 0.050084\n",
      ">> Epoch 6 finished \tANN training loss 0.049504\n",
      ">> Epoch 7 finished \tANN training loss 0.048930\n",
      ">> Epoch 8 finished \tANN training loss 0.048350\n",
      ">> Epoch 9 finished \tANN training loss 0.047797\n",
      ">> Epoch 10 finished \tANN training loss 0.047233\n",
      ">> Epoch 11 finished \tANN training loss 0.046694\n",
      ">> Epoch 12 finished \tANN training loss 0.046156\n",
      ">> Epoch 13 finished \tANN training loss 0.045627\n",
      ">> Epoch 14 finished \tANN training loss 0.045095\n",
      ">> Epoch 15 finished \tANN training loss 0.044580\n",
      ">> Epoch 16 finished \tANN training loss 0.044058\n",
      ">> Epoch 17 finished \tANN training loss 0.043554\n",
      ">> Epoch 18 finished \tANN training loss 0.043063\n",
      ">> Epoch 19 finished \tANN training loss 0.042570\n",
      ">> Epoch 20 finished \tANN training loss 0.042086\n",
      ">> Epoch 21 finished \tANN training loss 0.041593\n",
      ">> Epoch 22 finished \tANN training loss 0.041127\n",
      ">> Epoch 23 finished \tANN training loss 0.040650\n",
      ">> Epoch 24 finished \tANN training loss 0.040182\n",
      ">> Epoch 25 finished \tANN training loss 0.039723\n",
      ">> Epoch 26 finished \tANN training loss 0.039277\n",
      ">> Epoch 27 finished \tANN training loss 0.038829\n",
      ">> Epoch 28 finished \tANN training loss 0.038378\n",
      ">> Epoch 29 finished \tANN training loss 0.037950\n",
      ">> Epoch 30 finished \tANN training loss 0.037517\n",
      ">> Epoch 31 finished \tANN training loss 0.037089\n",
      ">> Epoch 32 finished \tANN training loss 0.036687\n",
      ">> Epoch 33 finished \tANN training loss 0.036264\n",
      ">> Epoch 34 finished \tANN training loss 0.035840\n",
      ">> Epoch 35 finished \tANN training loss 0.035444\n",
      ">> Epoch 36 finished \tANN training loss 0.035033\n",
      ">> Epoch 37 finished \tANN training loss 0.034656\n",
      ">> Epoch 38 finished \tANN training loss 0.034266\n",
      ">> Epoch 39 finished \tANN training loss 0.033891\n",
      ">> Epoch 40 finished \tANN training loss 0.033510\n",
      ">> Epoch 41 finished \tANN training loss 0.033154\n",
      ">> Epoch 42 finished \tANN training loss 0.032777\n",
      ">> Epoch 43 finished \tANN training loss 0.032415\n",
      ">> Epoch 44 finished \tANN training loss 0.032059\n",
      ">> Epoch 45 finished \tANN training loss 0.031706\n",
      ">> Epoch 46 finished \tANN training loss 0.031371\n",
      ">> Epoch 47 finished \tANN training loss 0.031026\n",
      ">> Epoch 48 finished \tANN training loss 0.030689\n",
      ">> Epoch 49 finished \tANN training loss 0.030367\n",
      ">> Epoch 50 finished \tANN training loss 0.030042\n",
      ">> Epoch 51 finished \tANN training loss 0.029709\n",
      ">> Epoch 52 finished \tANN training loss 0.029405\n",
      ">> Epoch 53 finished \tANN training loss 0.029076\n",
      ">> Epoch 54 finished \tANN training loss 0.028786\n",
      ">> Epoch 55 finished \tANN training loss 0.028489\n",
      ">> Epoch 56 finished \tANN training loss 0.028196\n",
      ">> Epoch 57 finished \tANN training loss 0.027904\n",
      ">> Epoch 58 finished \tANN training loss 0.027623\n",
      ">> Epoch 59 finished \tANN training loss 0.027329\n",
      ">> Epoch 60 finished \tANN training loss 0.027065\n",
      ">> Epoch 61 finished \tANN training loss 0.026791\n",
      ">> Epoch 62 finished \tANN training loss 0.026518\n",
      ">> Epoch 63 finished \tANN training loss 0.026261\n",
      ">> Epoch 64 finished \tANN training loss 0.026004\n",
      ">> Epoch 65 finished \tANN training loss 0.025750\n",
      ">> Epoch 66 finished \tANN training loss 0.025503\n",
      ">> Epoch 67 finished \tANN training loss 0.025258\n",
      ">> Epoch 68 finished \tANN training loss 0.025016\n",
      ">> Epoch 69 finished \tANN training loss 0.024781\n",
      ">> Epoch 70 finished \tANN training loss 0.024546\n",
      ">> Epoch 71 finished \tANN training loss 0.024322\n",
      ">> Epoch 72 finished \tANN training loss 0.024103\n",
      ">> Epoch 73 finished \tANN training loss 0.023888\n",
      ">> Epoch 74 finished \tANN training loss 0.023668\n",
      ">> Epoch 75 finished \tANN training loss 0.023463\n",
      ">> Epoch 76 finished \tANN training loss 0.023258\n",
      ">> Epoch 77 finished \tANN training loss 0.023055\n",
      ">> Epoch 78 finished \tANN training loss 0.022857\n",
      ">> Epoch 79 finished \tANN training loss 0.022665\n",
      ">> Epoch 80 finished \tANN training loss 0.022469\n",
      ">> Epoch 81 finished \tANN training loss 0.022288\n",
      ">> Epoch 82 finished \tANN training loss 0.022107\n",
      ">> Epoch 83 finished \tANN training loss 0.021923\n",
      ">> Epoch 84 finished \tANN training loss 0.021754\n",
      ">> Epoch 85 finished \tANN training loss 0.021583\n",
      ">> Epoch 86 finished \tANN training loss 0.021413\n",
      ">> Epoch 87 finished \tANN training loss 0.021246\n",
      ">> Epoch 88 finished \tANN training loss 0.021079\n",
      ">> Epoch 89 finished \tANN training loss 0.020926\n",
      ">> Epoch 90 finished \tANN training loss 0.020774\n",
      ">> Epoch 91 finished \tANN training loss 0.020624\n",
      ">> Epoch 92 finished \tANN training loss 0.020475\n",
      ">> Epoch 93 finished \tANN training loss 0.020326\n",
      ">> Epoch 94 finished \tANN training loss 0.020184\n",
      ">> Epoch 95 finished \tANN training loss 0.020045\n",
      ">> Epoch 96 finished \tANN training loss 0.019910\n",
      ">> Epoch 97 finished \tANN training loss 0.019778\n",
      ">> Epoch 98 finished \tANN training loss 0.019648\n",
      ">> Epoch 99 finished \tANN training loss 0.019513\n",
      ">> Epoch 100 finished \tANN training loss 0.019385\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GridSearch: 100%|██████████| 1/1 [54:50<00:00, 3290.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "{'best_result': {'time_window': 12, 'RMSE': 129.65931004397413}, 'model': SupervisedDBNRegression()}\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.357252\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.389214\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.401797\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.395128\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.384870\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.369401\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.353434\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.338423\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.322757\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.307343\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.508939\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.498070\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.495561\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.492174\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.488796\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.485197\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.481258\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.477751\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.473189\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.469330\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.072957\n",
      ">> Epoch 2 finished \tANN training loss 0.063756\n",
      ">> Epoch 3 finished \tANN training loss 0.062888\n",
      ">> Epoch 4 finished \tANN training loss 0.062062\n",
      ">> Epoch 5 finished \tANN training loss 0.061244\n",
      ">> Epoch 6 finished \tANN training loss 0.060440\n",
      ">> Epoch 7 finished \tANN training loss 0.059640\n",
      ">> Epoch 8 finished \tANN training loss 0.058872\n",
      ">> Epoch 9 finished \tANN training loss 0.058117\n",
      ">> Epoch 10 finished \tANN training loss 0.057372\n",
      ">> Epoch 11 finished \tANN training loss 0.056643\n",
      ">> Epoch 12 finished \tANN training loss 0.055937\n",
      ">> Epoch 13 finished \tANN training loss 0.055229\n",
      ">> Epoch 14 finished \tANN training loss 0.054542\n",
      ">> Epoch 15 finished \tANN training loss 0.053868\n",
      ">> Epoch 16 finished \tANN training loss 0.053213\n",
      ">> Epoch 17 finished \tANN training loss 0.052559\n",
      ">> Epoch 18 finished \tANN training loss 0.051919\n",
      ">> Epoch 19 finished \tANN training loss 0.051291\n",
      ">> Epoch 20 finished \tANN training loss 0.050677\n",
      ">> Epoch 21 finished \tANN training loss 0.050069\n",
      ">> Epoch 22 finished \tANN training loss 0.049461\n",
      ">> Epoch 23 finished \tANN training loss 0.048902\n",
      ">> Epoch 24 finished \tANN training loss 0.048319\n",
      ">> Epoch 25 finished \tANN training loss 0.047751\n",
      ">> Epoch 26 finished \tANN training loss 0.047193\n",
      ">> Epoch 27 finished \tANN training loss 0.046648\n",
      ">> Epoch 28 finished \tANN training loss 0.046106\n",
      ">> Epoch 29 finished \tANN training loss 0.045583\n",
      ">> Epoch 30 finished \tANN training loss 0.045063\n",
      ">> Epoch 31 finished \tANN training loss 0.044549\n",
      ">> Epoch 32 finished \tANN training loss 0.044045\n",
      ">> Epoch 33 finished \tANN training loss 0.043560\n",
      ">> Epoch 34 finished \tANN training loss 0.043066\n",
      ">> Epoch 35 finished \tANN training loss 0.042598\n",
      ">> Epoch 36 finished \tANN training loss 0.042126\n",
      ">> Epoch 37 finished \tANN training loss 0.041670\n",
      ">> Epoch 38 finished \tANN training loss 0.041206\n",
      ">> Epoch 39 finished \tANN training loss 0.040758\n",
      ">> Epoch 40 finished \tANN training loss 0.040315\n",
      ">> Epoch 41 finished \tANN training loss 0.039895\n",
      ">> Epoch 42 finished \tANN training loss 0.039470\n",
      ">> Epoch 43 finished \tANN training loss 0.039052\n",
      ">> Epoch 44 finished \tANN training loss 0.038642\n",
      ">> Epoch 45 finished \tANN training loss 0.038234\n",
      ">> Epoch 46 finished \tANN training loss 0.037840\n",
      ">> Epoch 47 finished \tANN training loss 0.037447\n",
      ">> Epoch 48 finished \tANN training loss 0.037063\n",
      ">> Epoch 49 finished \tANN training loss 0.036689\n",
      ">> Epoch 50 finished \tANN training loss 0.036328\n",
      ">> Epoch 51 finished \tANN training loss 0.035952\n",
      ">> Epoch 52 finished \tANN training loss 0.035605\n",
      ">> Epoch 53 finished \tANN training loss 0.035253\n",
      ">> Epoch 54 finished \tANN training loss 0.034914\n",
      ">> Epoch 55 finished \tANN training loss 0.034577\n",
      ">> Epoch 56 finished \tANN training loss 0.034235\n",
      ">> Epoch 57 finished \tANN training loss 0.033924\n",
      ">> Epoch 58 finished \tANN training loss 0.033604\n",
      ">> Epoch 59 finished \tANN training loss 0.033281\n",
      ">> Epoch 60 finished \tANN training loss 0.032989\n",
      ">> Epoch 61 finished \tANN training loss 0.032682\n",
      ">> Epoch 62 finished \tANN training loss 0.032388\n",
      ">> Epoch 63 finished \tANN training loss 0.032103\n",
      ">> Epoch 64 finished \tANN training loss 0.031817\n",
      ">> Epoch 65 finished \tANN training loss 0.031538\n",
      ">> Epoch 66 finished \tANN training loss 0.031272\n",
      ">> Epoch 67 finished \tANN training loss 0.030999\n",
      ">> Epoch 68 finished \tANN training loss 0.030749\n",
      ">> Epoch 69 finished \tANN training loss 0.030488\n",
      ">> Epoch 70 finished \tANN training loss 0.030240\n",
      ">> Epoch 71 finished \tANN training loss 0.029997\n",
      ">> Epoch 72 finished \tANN training loss 0.029756\n",
      ">> Epoch 73 finished \tANN training loss 0.029519\n",
      ">> Epoch 74 finished \tANN training loss 0.029284\n",
      ">> Epoch 75 finished \tANN training loss 0.029067\n",
      ">> Epoch 76 finished \tANN training loss 0.028840\n",
      ">> Epoch 77 finished \tANN training loss 0.028627\n",
      ">> Epoch 78 finished \tANN training loss 0.028418\n",
      ">> Epoch 79 finished \tANN training loss 0.028209\n",
      ">> Epoch 80 finished \tANN training loss 0.028009\n",
      ">> Epoch 81 finished \tANN training loss 0.027795\n",
      ">> Epoch 82 finished \tANN training loss 0.027611\n",
      ">> Epoch 83 finished \tANN training loss 0.027428\n",
      ">> Epoch 84 finished \tANN training loss 0.027240\n",
      ">> Epoch 85 finished \tANN training loss 0.027060\n",
      ">> Epoch 86 finished \tANN training loss 0.026881\n",
      ">> Epoch 87 finished \tANN training loss 0.026701\n",
      ">> Epoch 88 finished \tANN training loss 0.026528\n",
      ">> Epoch 89 finished \tANN training loss 0.026378\n",
      ">> Epoch 90 finished \tANN training loss 0.026204\n",
      ">> Epoch 91 finished \tANN training loss 0.026037\n",
      ">> Epoch 92 finished \tANN training loss 0.025891\n",
      ">> Epoch 93 finished \tANN training loss 0.025741\n",
      ">> Epoch 94 finished \tANN training loss 0.025593\n",
      ">> Epoch 95 finished \tANN training loss 0.025451\n",
      ">> Epoch 96 finished \tANN training loss 0.025308\n",
      ">> Epoch 97 finished \tANN training loss 0.025169\n",
      ">> Epoch 98 finished \tANN training loss 0.025034\n",
      ">> Epoch 99 finished \tANN training loss 0.024901\n",
      ">> Epoch 100 finished \tANN training loss 0.024764\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.377530\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.426303\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.440836\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.437821\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.426209\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.412261\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.396622\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.379477\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.363854\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.349708\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.446252\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.438455\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.436183\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.433471\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.430784\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.427698\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.424568\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.421621\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.418230\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.414871\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.078563\n",
      ">> Epoch 2 finished \tANN training loss 0.070836\n",
      ">> Epoch 3 finished \tANN training loss 0.069570\n",
      ">> Epoch 4 finished \tANN training loss 0.068393\n",
      ">> Epoch 5 finished \tANN training loss 0.067233\n",
      ">> Epoch 6 finished \tANN training loss 0.066087\n",
      ">> Epoch 7 finished \tANN training loss 0.064985\n",
      ">> Epoch 8 finished \tANN training loss 0.063927\n",
      ">> Epoch 9 finished \tANN training loss 0.062872\n",
      ">> Epoch 10 finished \tANN training loss 0.061856\n",
      ">> Epoch 11 finished \tANN training loss 0.060870\n",
      ">> Epoch 12 finished \tANN training loss 0.059912\n",
      ">> Epoch 13 finished \tANN training loss 0.058945\n",
      ">> Epoch 14 finished \tANN training loss 0.058044\n",
      ">> Epoch 15 finished \tANN training loss 0.057154\n",
      ">> Epoch 16 finished \tANN training loss 0.056274\n",
      ">> Epoch 17 finished \tANN training loss 0.055408\n",
      ">> Epoch 18 finished \tANN training loss 0.054571\n",
      ">> Epoch 19 finished \tANN training loss 0.053757\n",
      ">> Epoch 20 finished \tANN training loss 0.052946\n",
      ">> Epoch 21 finished \tANN training loss 0.052172\n",
      ">> Epoch 22 finished \tANN training loss 0.051419\n",
      ">> Epoch 23 finished \tANN training loss 0.050669\n",
      ">> Epoch 24 finished \tANN training loss 0.049915\n",
      ">> Epoch 25 finished \tANN training loss 0.049202\n",
      ">> Epoch 26 finished \tANN training loss 0.048511\n",
      ">> Epoch 27 finished \tANN training loss 0.047804\n",
      ">> Epoch 28 finished \tANN training loss 0.047142\n",
      ">> Epoch 29 finished \tANN training loss 0.046479\n",
      ">> Epoch 30 finished \tANN training loss 0.045843\n",
      ">> Epoch 31 finished \tANN training loss 0.045193\n",
      ">> Epoch 32 finished \tANN training loss 0.044572\n",
      ">> Epoch 33 finished \tANN training loss 0.043972\n",
      ">> Epoch 34 finished \tANN training loss 0.043379\n",
      ">> Epoch 35 finished \tANN training loss 0.042797\n",
      ">> Epoch 36 finished \tANN training loss 0.042218\n",
      ">> Epoch 37 finished \tANN training loss 0.041661\n",
      ">> Epoch 38 finished \tANN training loss 0.041092\n",
      ">> Epoch 39 finished \tANN training loss 0.040574\n",
      ">> Epoch 40 finished \tANN training loss 0.040036\n",
      ">> Epoch 41 finished \tANN training loss 0.039509\n",
      ">> Epoch 42 finished \tANN training loss 0.038999\n",
      ">> Epoch 43 finished \tANN training loss 0.038523\n",
      ">> Epoch 44 finished \tANN training loss 0.038026\n",
      ">> Epoch 45 finished \tANN training loss 0.037545\n",
      ">> Epoch 46 finished \tANN training loss 0.037071\n",
      ">> Epoch 47 finished \tANN training loss 0.036604\n",
      ">> Epoch 48 finished \tANN training loss 0.036167\n",
      ">> Epoch 49 finished \tANN training loss 0.035728\n",
      ">> Epoch 50 finished \tANN training loss 0.035284\n",
      ">> Epoch 51 finished \tANN training loss 0.034872\n",
      ">> Epoch 52 finished \tANN training loss 0.034435\n",
      ">> Epoch 53 finished \tANN training loss 0.034041\n",
      ">> Epoch 54 finished \tANN training loss 0.033644\n",
      ">> Epoch 55 finished \tANN training loss 0.033254\n",
      ">> Epoch 56 finished \tANN training loss 0.032868\n",
      ">> Epoch 57 finished \tANN training loss 0.032493\n",
      ">> Epoch 58 finished \tANN training loss 0.032129\n",
      ">> Epoch 59 finished \tANN training loss 0.031770\n",
      ">> Epoch 60 finished \tANN training loss 0.031412\n",
      ">> Epoch 61 finished \tANN training loss 0.031063\n",
      ">> Epoch 62 finished \tANN training loss 0.030734\n",
      ">> Epoch 63 finished \tANN training loss 0.030404\n",
      ">> Epoch 64 finished \tANN training loss 0.030080\n",
      ">> Epoch 65 finished \tANN training loss 0.029762\n",
      ">> Epoch 66 finished \tANN training loss 0.029445\n",
      ">> Epoch 67 finished \tANN training loss 0.029144\n",
      ">> Epoch 68 finished \tANN training loss 0.028848\n",
      ">> Epoch 69 finished \tANN training loss 0.028554\n",
      ">> Epoch 70 finished \tANN training loss 0.028277\n",
      ">> Epoch 71 finished \tANN training loss 0.027998\n",
      ">> Epoch 72 finished \tANN training loss 0.027730\n",
      ">> Epoch 73 finished \tANN training loss 0.027461\n",
      ">> Epoch 74 finished \tANN training loss 0.027196\n",
      ">> Epoch 75 finished \tANN training loss 0.026950\n",
      ">> Epoch 76 finished \tANN training loss 0.026688\n",
      ">> Epoch 77 finished \tANN training loss 0.026451\n",
      ">> Epoch 78 finished \tANN training loss 0.026207\n",
      ">> Epoch 79 finished \tANN training loss 0.025966\n",
      ">> Epoch 80 finished \tANN training loss 0.025747\n",
      ">> Epoch 81 finished \tANN training loss 0.025523\n",
      ">> Epoch 82 finished \tANN training loss 0.025311\n",
      ">> Epoch 83 finished \tANN training loss 0.025102\n",
      ">> Epoch 84 finished \tANN training loss 0.024890\n",
      ">> Epoch 85 finished \tANN training loss 0.024686\n",
      ">> Epoch 86 finished \tANN training loss 0.024482\n",
      ">> Epoch 87 finished \tANN training loss 0.024288\n",
      ">> Epoch 88 finished \tANN training loss 0.024102\n",
      ">> Epoch 89 finished \tANN training loss 0.023901\n",
      ">> Epoch 90 finished \tANN training loss 0.023730\n",
      ">> Epoch 91 finished \tANN training loss 0.023551\n",
      ">> Epoch 92 finished \tANN training loss 0.023377\n",
      ">> Epoch 93 finished \tANN training loss 0.023200\n",
      ">> Epoch 94 finished \tANN training loss 0.023040\n",
      ">> Epoch 95 finished \tANN training loss 0.022875\n",
      ">> Epoch 96 finished \tANN training loss 0.022713\n",
      ">> Epoch 97 finished \tANN training loss 0.022556\n",
      ">> Epoch 98 finished \tANN training loss 0.022401\n",
      ">> Epoch 99 finished \tANN training loss 0.022252\n",
      ">> Epoch 100 finished \tANN training loss 0.022109\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.309916\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.359548\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.378400\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.377610\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.368079\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.353312\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.339055\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.324910\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.309257\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.294562\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.525511\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.514402\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.511952\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.508593\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.505462\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.501566\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.497718\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.494083\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.489919\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.485913\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.091781\n",
      ">> Epoch 2 finished \tANN training loss 0.068576\n",
      ">> Epoch 3 finished \tANN training loss 0.067586\n",
      ">> Epoch 4 finished \tANN training loss 0.066640\n",
      ">> Epoch 5 finished \tANN training loss 0.065690\n",
      ">> Epoch 6 finished \tANN training loss 0.064771\n",
      ">> Epoch 7 finished \tANN training loss 0.063877\n",
      ">> Epoch 8 finished \tANN training loss 0.063006\n",
      ">> Epoch 9 finished \tANN training loss 0.062150\n",
      ">> Epoch 10 finished \tANN training loss 0.061304\n",
      ">> Epoch 11 finished \tANN training loss 0.060500\n",
      ">> Epoch 12 finished \tANN training loss 0.059700\n",
      ">> Epoch 13 finished \tANN training loss 0.058938\n",
      ">> Epoch 14 finished \tANN training loss 0.058183\n",
      ">> Epoch 15 finished \tANN training loss 0.057422\n",
      ">> Epoch 16 finished \tANN training loss 0.056713\n",
      ">> Epoch 17 finished \tANN training loss 0.056017\n",
      ">> Epoch 18 finished \tANN training loss 0.055308\n",
      ">> Epoch 19 finished \tANN training loss 0.054646\n",
      ">> Epoch 20 finished \tANN training loss 0.053961\n",
      ">> Epoch 21 finished \tANN training loss 0.053317\n",
      ">> Epoch 22 finished \tANN training loss 0.052691\n",
      ">> Epoch 23 finished \tANN training loss 0.052041\n",
      ">> Epoch 24 finished \tANN training loss 0.051440\n",
      ">> Epoch 25 finished \tANN training loss 0.050827\n",
      ">> Epoch 26 finished \tANN training loss 0.050228\n",
      ">> Epoch 27 finished \tANN training loss 0.049666\n",
      ">> Epoch 28 finished \tANN training loss 0.049090\n",
      ">> Epoch 29 finished \tANN training loss 0.048530\n",
      ">> Epoch 30 finished \tANN training loss 0.047975\n",
      ">> Epoch 31 finished \tANN training loss 0.047447\n",
      ">> Epoch 32 finished \tANN training loss 0.046906\n",
      ">> Epoch 33 finished \tANN training loss 0.046393\n",
      ">> Epoch 34 finished \tANN training loss 0.045873\n",
      ">> Epoch 35 finished \tANN training loss 0.045388\n",
      ">> Epoch 36 finished \tANN training loss 0.044894\n",
      ">> Epoch 37 finished \tANN training loss 0.044395\n",
      ">> Epoch 38 finished \tANN training loss 0.043929\n",
      ">> Epoch 39 finished \tANN training loss 0.043465\n",
      ">> Epoch 40 finished \tANN training loss 0.042999\n",
      ">> Epoch 41 finished \tANN training loss 0.042542\n",
      ">> Epoch 42 finished \tANN training loss 0.042094\n",
      ">> Epoch 43 finished \tANN training loss 0.041663\n",
      ">> Epoch 44 finished \tANN training loss 0.041243\n",
      ">> Epoch 45 finished \tANN training loss 0.040805\n",
      ">> Epoch 46 finished \tANN training loss 0.040411\n",
      ">> Epoch 47 finished \tANN training loss 0.039996\n",
      ">> Epoch 48 finished \tANN training loss 0.039597\n",
      ">> Epoch 49 finished \tANN training loss 0.039200\n",
      ">> Epoch 50 finished \tANN training loss 0.038812\n",
      ">> Epoch 51 finished \tANN training loss 0.038430\n",
      ">> Epoch 52 finished \tANN training loss 0.038058\n",
      ">> Epoch 53 finished \tANN training loss 0.037696\n",
      ">> Epoch 54 finished \tANN training loss 0.037329\n",
      ">> Epoch 55 finished \tANN training loss 0.036974\n",
      ">> Epoch 56 finished \tANN training loss 0.036631\n",
      ">> Epoch 57 finished \tANN training loss 0.036268\n",
      ">> Epoch 58 finished \tANN training loss 0.035955\n",
      ">> Epoch 59 finished \tANN training loss 0.035621\n",
      ">> Epoch 60 finished \tANN training loss 0.035282\n",
      ">> Epoch 61 finished \tANN training loss 0.034975\n",
      ">> Epoch 62 finished \tANN training loss 0.034666\n",
      ">> Epoch 63 finished \tANN training loss 0.034340\n",
      ">> Epoch 64 finished \tANN training loss 0.034052\n",
      ">> Epoch 65 finished \tANN training loss 0.033757\n",
      ">> Epoch 66 finished \tANN training loss 0.033466\n",
      ">> Epoch 67 finished \tANN training loss 0.033176\n",
      ">> Epoch 68 finished \tANN training loss 0.032899\n",
      ">> Epoch 69 finished \tANN training loss 0.032618\n",
      ">> Epoch 70 finished \tANN training loss 0.032342\n",
      ">> Epoch 71 finished \tANN training loss 0.032072\n",
      ">> Epoch 72 finished \tANN training loss 0.031820\n",
      ">> Epoch 73 finished \tANN training loss 0.031563\n",
      ">> Epoch 74 finished \tANN training loss 0.031313\n",
      ">> Epoch 75 finished \tANN training loss 0.031068\n",
      ">> Epoch 76 finished \tANN training loss 0.030826\n",
      ">> Epoch 77 finished \tANN training loss 0.030583\n",
      ">> Epoch 78 finished \tANN training loss 0.030356\n",
      ">> Epoch 79 finished \tANN training loss 0.030124\n",
      ">> Epoch 80 finished \tANN training loss 0.029893\n",
      ">> Epoch 81 finished \tANN training loss 0.029676\n",
      ">> Epoch 82 finished \tANN training loss 0.029453\n",
      ">> Epoch 83 finished \tANN training loss 0.029255\n",
      ">> Epoch 84 finished \tANN training loss 0.029044\n",
      ">> Epoch 85 finished \tANN training loss 0.028840\n",
      ">> Epoch 86 finished \tANN training loss 0.028639\n",
      ">> Epoch 87 finished \tANN training loss 0.028443\n",
      ">> Epoch 88 finished \tANN training loss 0.028248\n",
      ">> Epoch 89 finished \tANN training loss 0.028055\n",
      ">> Epoch 90 finished \tANN training loss 0.027874\n",
      ">> Epoch 91 finished \tANN training loss 0.027692\n",
      ">> Epoch 92 finished \tANN training loss 0.027511\n",
      ">> Epoch 93 finished \tANN training loss 0.027341\n",
      ">> Epoch 94 finished \tANN training loss 0.027161\n",
      ">> Epoch 95 finished \tANN training loss 0.026995\n",
      ">> Epoch 96 finished \tANN training loss 0.026831\n",
      ">> Epoch 97 finished \tANN training loss 0.026671\n",
      ">> Epoch 98 finished \tANN training loss 0.026502\n",
      ">> Epoch 99 finished \tANN training loss 0.026354\n",
      ">> Epoch 100 finished \tANN training loss 0.026197\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.346040\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.400913\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.425987\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.426639\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.416960\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.405800\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.390482\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.373049\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.358277\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.343092\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.463216\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.451592\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.449328\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.446763\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.444062\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.441203\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.438529\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.435223\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.432214\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.428971\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.064223\n",
      ">> Epoch 2 finished \tANN training loss 0.058848\n",
      ">> Epoch 3 finished \tANN training loss 0.058019\n",
      ">> Epoch 4 finished \tANN training loss 0.057188\n",
      ">> Epoch 5 finished \tANN training loss 0.056397\n",
      ">> Epoch 6 finished \tANN training loss 0.055604\n",
      ">> Epoch 7 finished \tANN training loss 0.054825\n",
      ">> Epoch 8 finished \tANN training loss 0.054091\n",
      ">> Epoch 9 finished \tANN training loss 0.053351\n",
      ">> Epoch 10 finished \tANN training loss 0.052637\n",
      ">> Epoch 11 finished \tANN training loss 0.051926\n",
      ">> Epoch 12 finished \tANN training loss 0.051245\n",
      ">> Epoch 13 finished \tANN training loss 0.050562\n",
      ">> Epoch 14 finished \tANN training loss 0.049880\n",
      ">> Epoch 15 finished \tANN training loss 0.049261\n",
      ">> Epoch 16 finished \tANN training loss 0.048610\n",
      ">> Epoch 17 finished \tANN training loss 0.047984\n",
      ">> Epoch 18 finished \tANN training loss 0.047376\n",
      ">> Epoch 19 finished \tANN training loss 0.046771\n",
      ">> Epoch 20 finished \tANN training loss 0.046182\n",
      ">> Epoch 21 finished \tANN training loss 0.045591\n",
      ">> Epoch 22 finished \tANN training loss 0.045042\n",
      ">> Epoch 23 finished \tANN training loss 0.044485\n",
      ">> Epoch 24 finished \tANN training loss 0.043939\n",
      ">> Epoch 25 finished \tANN training loss 0.043405\n",
      ">> Epoch 26 finished \tANN training loss 0.042879\n",
      ">> Epoch 27 finished \tANN training loss 0.042367\n",
      ">> Epoch 28 finished \tANN training loss 0.041867\n",
      ">> Epoch 29 finished \tANN training loss 0.041363\n",
      ">> Epoch 30 finished \tANN training loss 0.040881\n",
      ">> Epoch 31 finished \tANN training loss 0.040406\n",
      ">> Epoch 32 finished \tANN training loss 0.039939\n",
      ">> Epoch 33 finished \tANN training loss 0.039477\n",
      ">> Epoch 34 finished \tANN training loss 0.039037\n",
      ">> Epoch 35 finished \tANN training loss 0.038593\n",
      ">> Epoch 36 finished \tANN training loss 0.038166\n",
      ">> Epoch 37 finished \tANN training loss 0.037743\n",
      ">> Epoch 38 finished \tANN training loss 0.037316\n",
      ">> Epoch 39 finished \tANN training loss 0.036926\n",
      ">> Epoch 40 finished \tANN training loss 0.036524\n",
      ">> Epoch 41 finished \tANN training loss 0.036142\n",
      ">> Epoch 42 finished \tANN training loss 0.035751\n",
      ">> Epoch 43 finished \tANN training loss 0.035385\n",
      ">> Epoch 44 finished \tANN training loss 0.035021\n",
      ">> Epoch 45 finished \tANN training loss 0.034663\n",
      ">> Epoch 46 finished \tANN training loss 0.034315\n",
      ">> Epoch 47 finished \tANN training loss 0.033976\n",
      ">> Epoch 48 finished \tANN training loss 0.033618\n",
      ">> Epoch 49 finished \tANN training loss 0.033309\n",
      ">> Epoch 50 finished \tANN training loss 0.032981\n",
      ">> Epoch 51 finished \tANN training loss 0.032673\n",
      ">> Epoch 52 finished \tANN training loss 0.032369\n",
      ">> Epoch 53 finished \tANN training loss 0.032058\n",
      ">> Epoch 54 finished \tANN training loss 0.031770\n",
      ">> Epoch 55 finished \tANN training loss 0.031478\n",
      ">> Epoch 56 finished \tANN training loss 0.031201\n",
      ">> Epoch 57 finished \tANN training loss 0.030925\n",
      ">> Epoch 58 finished \tANN training loss 0.030661\n",
      ">> Epoch 59 finished \tANN training loss 0.030388\n",
      ">> Epoch 60 finished \tANN training loss 0.030142\n",
      ">> Epoch 61 finished \tANN training loss 0.029888\n",
      ">> Epoch 62 finished \tANN training loss 0.029645\n",
      ">> Epoch 63 finished \tANN training loss 0.029395\n",
      ">> Epoch 64 finished \tANN training loss 0.029171\n",
      ">> Epoch 65 finished \tANN training loss 0.028944\n",
      ">> Epoch 66 finished \tANN training loss 0.028723\n",
      ">> Epoch 67 finished \tANN training loss 0.028500\n",
      ">> Epoch 68 finished \tANN training loss 0.028283\n",
      ">> Epoch 69 finished \tANN training loss 0.028074\n",
      ">> Epoch 70 finished \tANN training loss 0.027879\n",
      ">> Epoch 71 finished \tANN training loss 0.027675\n",
      ">> Epoch 72 finished \tANN training loss 0.027483\n",
      ">> Epoch 73 finished \tANN training loss 0.027297\n",
      ">> Epoch 74 finished \tANN training loss 0.027115\n",
      ">> Epoch 75 finished \tANN training loss 0.026934\n",
      ">> Epoch 76 finished \tANN training loss 0.026755\n",
      ">> Epoch 77 finished \tANN training loss 0.026583\n",
      ">> Epoch 78 finished \tANN training loss 0.026418\n",
      ">> Epoch 79 finished \tANN training loss 0.026257\n",
      ">> Epoch 80 finished \tANN training loss 0.026095\n",
      ">> Epoch 81 finished \tANN training loss 0.025931\n",
      ">> Epoch 82 finished \tANN training loss 0.025787\n",
      ">> Epoch 83 finished \tANN training loss 0.025633\n",
      ">> Epoch 84 finished \tANN training loss 0.025498\n",
      ">> Epoch 85 finished \tANN training loss 0.025347\n",
      ">> Epoch 86 finished \tANN training loss 0.025215\n",
      ">> Epoch 87 finished \tANN training loss 0.025073\n",
      ">> Epoch 88 finished \tANN training loss 0.024939\n",
      ">> Epoch 89 finished \tANN training loss 0.024813\n",
      ">> Epoch 90 finished \tANN training loss 0.024686\n",
      ">> Epoch 91 finished \tANN training loss 0.024559\n",
      ">> Epoch 92 finished \tANN training loss 0.024441\n",
      ">> Epoch 93 finished \tANN training loss 0.024327\n",
      ">> Epoch 94 finished \tANN training loss 0.024206\n",
      ">> Epoch 95 finished \tANN training loss 0.024093\n",
      ">> Epoch 96 finished \tANN training loss 0.023982\n",
      ">> Epoch 97 finished \tANN training loss 0.023871\n",
      ">> Epoch 98 finished \tANN training loss 0.023769\n",
      ">> Epoch 99 finished \tANN training loss 0.023669\n",
      ">> Epoch 100 finished \tANN training loss 0.023570\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.370706\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.406801\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.424390\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.425315\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.414902\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.402661\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.385520\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.370376\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.353895\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.337834\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.470087\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.458069\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.455608\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.452793\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.449806\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.446444\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.443434\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.439733\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.436506\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.432542\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.075205\n",
      ">> Epoch 2 finished \tANN training loss 0.061080\n",
      ">> Epoch 3 finished \tANN training loss 0.060307\n",
      ">> Epoch 4 finished \tANN training loss 0.059555\n",
      ">> Epoch 5 finished \tANN training loss 0.058796\n",
      ">> Epoch 6 finished \tANN training loss 0.058074\n",
      ">> Epoch 7 finished \tANN training loss 0.057373\n",
      ">> Epoch 8 finished \tANN training loss 0.056665\n",
      ">> Epoch 9 finished \tANN training loss 0.055980\n",
      ">> Epoch 10 finished \tANN training loss 0.055300\n",
      ">> Epoch 11 finished \tANN training loss 0.054649\n",
      ">> Epoch 12 finished \tANN training loss 0.053987\n",
      ">> Epoch 13 finished \tANN training loss 0.053361\n",
      ">> Epoch 14 finished \tANN training loss 0.052730\n",
      ">> Epoch 15 finished \tANN training loss 0.052107\n",
      ">> Epoch 16 finished \tANN training loss 0.051508\n",
      ">> Epoch 17 finished \tANN training loss 0.050906\n",
      ">> Epoch 18 finished \tANN training loss 0.050318\n",
      ">> Epoch 19 finished \tANN training loss 0.049754\n",
      ">> Epoch 20 finished \tANN training loss 0.049177\n",
      ">> Epoch 21 finished \tANN training loss 0.048604\n",
      ">> Epoch 22 finished \tANN training loss 0.048074\n",
      ">> Epoch 23 finished \tANN training loss 0.047538\n",
      ">> Epoch 24 finished \tANN training loss 0.046998\n",
      ">> Epoch 25 finished \tANN training loss 0.046485\n",
      ">> Epoch 26 finished \tANN training loss 0.045959\n",
      ">> Epoch 27 finished \tANN training loss 0.045461\n",
      ">> Epoch 28 finished \tANN training loss 0.044962\n",
      ">> Epoch 29 finished \tANN training loss 0.044473\n",
      ">> Epoch 30 finished \tANN training loss 0.043987\n",
      ">> Epoch 31 finished \tANN training loss 0.043528\n",
      ">> Epoch 32 finished \tANN training loss 0.043056\n",
      ">> Epoch 33 finished \tANN training loss 0.042604\n",
      ">> Epoch 34 finished \tANN training loss 0.042159\n",
      ">> Epoch 35 finished \tANN training loss 0.041708\n",
      ">> Epoch 36 finished \tANN training loss 0.041279\n",
      ">> Epoch 37 finished \tANN training loss 0.040854\n",
      ">> Epoch 38 finished \tANN training loss 0.040424\n",
      ">> Epoch 39 finished \tANN training loss 0.040016\n",
      ">> Epoch 40 finished \tANN training loss 0.039599\n",
      ">> Epoch 41 finished \tANN training loss 0.039213\n",
      ">> Epoch 42 finished \tANN training loss 0.038818\n",
      ">> Epoch 43 finished \tANN training loss 0.038427\n",
      ">> Epoch 44 finished \tANN training loss 0.038048\n",
      ">> Epoch 45 finished \tANN training loss 0.037673\n",
      ">> Epoch 46 finished \tANN training loss 0.037306\n",
      ">> Epoch 47 finished \tANN training loss 0.036952\n",
      ">> Epoch 48 finished \tANN training loss 0.036592\n",
      ">> Epoch 49 finished \tANN training loss 0.036237\n",
      ">> Epoch 50 finished \tANN training loss 0.035897\n",
      ">> Epoch 51 finished \tANN training loss 0.035564\n",
      ">> Epoch 52 finished \tANN training loss 0.035238\n",
      ">> Epoch 53 finished \tANN training loss 0.034911\n",
      ">> Epoch 54 finished \tANN training loss 0.034601\n",
      ">> Epoch 55 finished \tANN training loss 0.034285\n",
      ">> Epoch 56 finished \tANN training loss 0.033977\n",
      ">> Epoch 57 finished \tANN training loss 0.033678\n",
      ">> Epoch 58 finished \tANN training loss 0.033383\n",
      ">> Epoch 59 finished \tANN training loss 0.033092\n",
      ">> Epoch 60 finished \tANN training loss 0.032798\n",
      ">> Epoch 61 finished \tANN training loss 0.032523\n",
      ">> Epoch 62 finished \tANN training loss 0.032239\n",
      ">> Epoch 63 finished \tANN training loss 0.031987\n",
      ">> Epoch 64 finished \tANN training loss 0.031719\n",
      ">> Epoch 65 finished \tANN training loss 0.031460\n",
      ">> Epoch 66 finished \tANN training loss 0.031209\n",
      ">> Epoch 67 finished \tANN training loss 0.030963\n",
      ">> Epoch 68 finished \tANN training loss 0.030710\n",
      ">> Epoch 69 finished \tANN training loss 0.030481\n",
      ">> Epoch 70 finished \tANN training loss 0.030242\n",
      ">> Epoch 71 finished \tANN training loss 0.030014\n",
      ">> Epoch 72 finished \tANN training loss 0.029777\n",
      ">> Epoch 73 finished \tANN training loss 0.029566\n",
      ">> Epoch 74 finished \tANN training loss 0.029354\n",
      ">> Epoch 75 finished \tANN training loss 0.029144\n",
      ">> Epoch 76 finished \tANN training loss 0.028938\n",
      ">> Epoch 77 finished \tANN training loss 0.028733\n",
      ">> Epoch 78 finished \tANN training loss 0.028536\n",
      ">> Epoch 79 finished \tANN training loss 0.028335\n",
      ">> Epoch 80 finished \tANN training loss 0.028146\n",
      ">> Epoch 81 finished \tANN training loss 0.027956\n",
      ">> Epoch 82 finished \tANN training loss 0.027769\n",
      ">> Epoch 83 finished \tANN training loss 0.027591\n",
      ">> Epoch 84 finished \tANN training loss 0.027415\n",
      ">> Epoch 85 finished \tANN training loss 0.027251\n",
      ">> Epoch 86 finished \tANN training loss 0.027079\n",
      ">> Epoch 87 finished \tANN training loss 0.026921\n",
      ">> Epoch 88 finished \tANN training loss 0.026753\n",
      ">> Epoch 89 finished \tANN training loss 0.026578\n",
      ">> Epoch 90 finished \tANN training loss 0.026447\n",
      ">> Epoch 91 finished \tANN training loss 0.026293\n",
      ">> Epoch 92 finished \tANN training loss 0.026142\n",
      ">> Epoch 93 finished \tANN training loss 0.025996\n",
      ">> Epoch 94 finished \tANN training loss 0.025855\n",
      ">> Epoch 95 finished \tANN training loss 0.025713\n",
      ">> Epoch 96 finished \tANN training loss 0.025568\n",
      ">> Epoch 97 finished \tANN training loss 0.025439\n",
      ">> Epoch 98 finished \tANN training loss 0.025306\n",
      ">> Epoch 99 finished \tANN training loss 0.025179\n",
      ">> Epoch 100 finished \tANN training loss 0.025056\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.392890\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.436155\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.452915\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.449228\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.439036\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.428410\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.413599\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.399588\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.383827\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.369927\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.450423\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.440775\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.439256\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.436973\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.434626\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.432225\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.429797\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.427286\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.424391\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.421801\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.074731\n",
      ">> Epoch 2 finished \tANN training loss 0.064222\n",
      ">> Epoch 3 finished \tANN training loss 0.063066\n",
      ">> Epoch 4 finished \tANN training loss 0.061998\n",
      ">> Epoch 5 finished \tANN training loss 0.060925\n",
      ">> Epoch 6 finished \tANN training loss 0.059915\n",
      ">> Epoch 7 finished \tANN training loss 0.058912\n",
      ">> Epoch 8 finished \tANN training loss 0.057922\n",
      ">> Epoch 9 finished \tANN training loss 0.056982\n",
      ">> Epoch 10 finished \tANN training loss 0.056071\n",
      ">> Epoch 11 finished \tANN training loss 0.055180\n",
      ">> Epoch 12 finished \tANN training loss 0.054283\n",
      ">> Epoch 13 finished \tANN training loss 0.053456\n",
      ">> Epoch 14 finished \tANN training loss 0.052625\n",
      ">> Epoch 15 finished \tANN training loss 0.051809\n",
      ">> Epoch 16 finished \tANN training loss 0.051025\n",
      ">> Epoch 17 finished \tANN training loss 0.050254\n",
      ">> Epoch 18 finished \tANN training loss 0.049503\n",
      ">> Epoch 19 finished \tANN training loss 0.048772\n",
      ">> Epoch 20 finished \tANN training loss 0.048058\n",
      ">> Epoch 21 finished \tANN training loss 0.047347\n",
      ">> Epoch 22 finished \tANN training loss 0.046677\n",
      ">> Epoch 23 finished \tANN training loss 0.046005\n",
      ">> Epoch 24 finished \tANN training loss 0.045360\n",
      ">> Epoch 25 finished \tANN training loss 0.044715\n",
      ">> Epoch 26 finished \tANN training loss 0.044099\n",
      ">> Epoch 27 finished \tANN training loss 0.043495\n",
      ">> Epoch 28 finished \tANN training loss 0.042895\n",
      ">> Epoch 29 finished \tANN training loss 0.042321\n",
      ">> Epoch 30 finished \tANN training loss 0.041739\n",
      ">> Epoch 31 finished \tANN training loss 0.041198\n",
      ">> Epoch 32 finished \tANN training loss 0.040658\n",
      ">> Epoch 33 finished \tANN training loss 0.040128\n",
      ">> Epoch 34 finished \tANN training loss 0.039606\n",
      ">> Epoch 35 finished \tANN training loss 0.039093\n",
      ">> Epoch 36 finished \tANN training loss 0.038618\n",
      ">> Epoch 37 finished \tANN training loss 0.038121\n",
      ">> Epoch 38 finished \tANN training loss 0.037653\n",
      ">> Epoch 39 finished \tANN training loss 0.037201\n",
      ">> Epoch 40 finished \tANN training loss 0.036737\n",
      ">> Epoch 41 finished \tANN training loss 0.036301\n",
      ">> Epoch 42 finished \tANN training loss 0.035873\n",
      ">> Epoch 43 finished \tANN training loss 0.035449\n",
      ">> Epoch 44 finished \tANN training loss 0.035036\n",
      ">> Epoch 45 finished \tANN training loss 0.034641\n",
      ">> Epoch 46 finished \tANN training loss 0.034244\n",
      ">> Epoch 47 finished \tANN training loss 0.033858\n",
      ">> Epoch 48 finished \tANN training loss 0.033485\n",
      ">> Epoch 49 finished \tANN training loss 0.033123\n",
      ">> Epoch 50 finished \tANN training loss 0.032764\n",
      ">> Epoch 51 finished \tANN training loss 0.032408\n",
      ">> Epoch 52 finished \tANN training loss 0.032075\n",
      ">> Epoch 53 finished \tANN training loss 0.031742\n",
      ">> Epoch 54 finished \tANN training loss 0.031415\n",
      ">> Epoch 55 finished \tANN training loss 0.031094\n",
      ">> Epoch 56 finished \tANN training loss 0.030777\n",
      ">> Epoch 57 finished \tANN training loss 0.030477\n",
      ">> Epoch 58 finished \tANN training loss 0.030183\n",
      ">> Epoch 59 finished \tANN training loss 0.029893\n",
      ">> Epoch 60 finished \tANN training loss 0.029616\n",
      ">> Epoch 61 finished \tANN training loss 0.029337\n",
      ">> Epoch 62 finished \tANN training loss 0.029064\n",
      ">> Epoch 63 finished \tANN training loss 0.028806\n",
      ">> Epoch 64 finished \tANN training loss 0.028550\n",
      ">> Epoch 65 finished \tANN training loss 0.028297\n",
      ">> Epoch 66 finished \tANN training loss 0.028052\n",
      ">> Epoch 67 finished \tANN training loss 0.027815\n",
      ">> Epoch 68 finished \tANN training loss 0.027583\n",
      ">> Epoch 69 finished \tANN training loss 0.027349\n",
      ">> Epoch 70 finished \tANN training loss 0.027135\n",
      ">> Epoch 71 finished \tANN training loss 0.026910\n",
      ">> Epoch 72 finished \tANN training loss 0.026707\n",
      ">> Epoch 73 finished \tANN training loss 0.026500\n",
      ">> Epoch 74 finished \tANN training loss 0.026297\n",
      ">> Epoch 75 finished \tANN training loss 0.026102\n",
      ">> Epoch 76 finished \tANN training loss 0.025913\n",
      ">> Epoch 77 finished \tANN training loss 0.025726\n",
      ">> Epoch 78 finished \tANN training loss 0.025545\n",
      ">> Epoch 79 finished \tANN training loss 0.025358\n",
      ">> Epoch 80 finished \tANN training loss 0.025184\n",
      ">> Epoch 81 finished \tANN training loss 0.025011\n",
      ">> Epoch 82 finished \tANN training loss 0.024847\n",
      ">> Epoch 83 finished \tANN training loss 0.024686\n",
      ">> Epoch 84 finished \tANN training loss 0.024527\n",
      ">> Epoch 85 finished \tANN training loss 0.024372\n",
      ">> Epoch 86 finished \tANN training loss 0.024208\n",
      ">> Epoch 87 finished \tANN training loss 0.024063\n",
      ">> Epoch 88 finished \tANN training loss 0.023924\n",
      ">> Epoch 89 finished \tANN training loss 0.023780\n",
      ">> Epoch 90 finished \tANN training loss 0.023639\n",
      ">> Epoch 91 finished \tANN training loss 0.023505\n",
      ">> Epoch 92 finished \tANN training loss 0.023377\n",
      ">> Epoch 93 finished \tANN training loss 0.023243\n",
      ">> Epoch 94 finished \tANN training loss 0.023116\n",
      ">> Epoch 95 finished \tANN training loss 0.022996\n",
      ">> Epoch 96 finished \tANN training loss 0.022875\n",
      ">> Epoch 97 finished \tANN training loss 0.022755\n",
      ">> Epoch 98 finished \tANN training loss 0.022636\n",
      ">> Epoch 99 finished \tANN training loss 0.022529\n",
      ">> Epoch 100 finished \tANN training loss 0.022412\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.370735\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.415658\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.433365\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.431967\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.422297\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.410049\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.395999\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.379001\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.364588\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.349426\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.452084\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.437214\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.435193\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.432178\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.429243\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.425900\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.422791\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.419205\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.415553\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.412152\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.061715\n",
      ">> Epoch 2 finished \tANN training loss 0.060926\n",
      ">> Epoch 3 finished \tANN training loss 0.060134\n",
      ">> Epoch 4 finished \tANN training loss 0.059410\n",
      ">> Epoch 5 finished \tANN training loss 0.058676\n",
      ">> Epoch 6 finished \tANN training loss 0.057965\n",
      ">> Epoch 7 finished \tANN training loss 0.057292\n",
      ">> Epoch 8 finished \tANN training loss 0.056595\n",
      ">> Epoch 9 finished \tANN training loss 0.055947\n",
      ">> Epoch 10 finished \tANN training loss 0.055296\n",
      ">> Epoch 11 finished \tANN training loss 0.054652\n",
      ">> Epoch 12 finished \tANN training loss 0.054039\n",
      ">> Epoch 13 finished \tANN training loss 0.053419\n",
      ">> Epoch 14 finished \tANN training loss 0.052823\n",
      ">> Epoch 15 finished \tANN training loss 0.052226\n",
      ">> Epoch 16 finished \tANN training loss 0.051650\n",
      ">> Epoch 17 finished \tANN training loss 0.051092\n",
      ">> Epoch 18 finished \tANN training loss 0.050538\n",
      ">> Epoch 19 finished \tANN training loss 0.049984\n",
      ">> Epoch 20 finished \tANN training loss 0.049450\n",
      ">> Epoch 21 finished \tANN training loss 0.048936\n",
      ">> Epoch 22 finished \tANN training loss 0.048409\n",
      ">> Epoch 23 finished \tANN training loss 0.047900\n",
      ">> Epoch 24 finished \tANN training loss 0.047400\n",
      ">> Epoch 25 finished \tANN training loss 0.046912\n",
      ">> Epoch 26 finished \tANN training loss 0.046428\n",
      ">> Epoch 27 finished \tANN training loss 0.045956\n",
      ">> Epoch 28 finished \tANN training loss 0.045465\n",
      ">> Epoch 29 finished \tANN training loss 0.045038\n",
      ">> Epoch 30 finished \tANN training loss 0.044556\n",
      ">> Epoch 31 finished \tANN training loss 0.044117\n",
      ">> Epoch 32 finished \tANN training loss 0.043675\n",
      ">> Epoch 33 finished \tANN training loss 0.043271\n",
      ">> Epoch 34 finished \tANN training loss 0.042839\n",
      ">> Epoch 35 finished \tANN training loss 0.042426\n",
      ">> Epoch 36 finished \tANN training loss 0.042011\n",
      ">> Epoch 37 finished \tANN training loss 0.041612\n",
      ">> Epoch 38 finished \tANN training loss 0.041217\n",
      ">> Epoch 39 finished \tANN training loss 0.040820\n",
      ">> Epoch 40 finished \tANN training loss 0.040431\n",
      ">> Epoch 41 finished \tANN training loss 0.040052\n",
      ">> Epoch 42 finished \tANN training loss 0.039679\n",
      ">> Epoch 43 finished \tANN training loss 0.039309\n",
      ">> Epoch 44 finished \tANN training loss 0.038948\n",
      ">> Epoch 45 finished \tANN training loss 0.038589\n",
      ">> Epoch 46 finished \tANN training loss 0.038239\n",
      ">> Epoch 47 finished \tANN training loss 0.037899\n",
      ">> Epoch 48 finished \tANN training loss 0.037547\n",
      ">> Epoch 49 finished \tANN training loss 0.037230\n",
      ">> Epoch 50 finished \tANN training loss 0.036891\n",
      ">> Epoch 51 finished \tANN training loss 0.036572\n",
      ">> Epoch 52 finished \tANN training loss 0.036249\n",
      ">> Epoch 53 finished \tANN training loss 0.035930\n",
      ">> Epoch 54 finished \tANN training loss 0.035630\n",
      ">> Epoch 55 finished \tANN training loss 0.035318\n",
      ">> Epoch 56 finished \tANN training loss 0.035026\n",
      ">> Epoch 57 finished \tANN training loss 0.034723\n",
      ">> Epoch 58 finished \tANN training loss 0.034434\n",
      ">> Epoch 59 finished \tANN training loss 0.034149\n",
      ">> Epoch 60 finished \tANN training loss 0.033866\n",
      ">> Epoch 61 finished \tANN training loss 0.033595\n",
      ">> Epoch 62 finished \tANN training loss 0.033327\n",
      ">> Epoch 63 finished \tANN training loss 0.033049\n",
      ">> Epoch 64 finished \tANN training loss 0.032790\n",
      ">> Epoch 65 finished \tANN training loss 0.032527\n",
      ">> Epoch 66 finished \tANN training loss 0.032272\n",
      ">> Epoch 67 finished \tANN training loss 0.032017\n",
      ">> Epoch 68 finished \tANN training loss 0.031772\n",
      ">> Epoch 69 finished \tANN training loss 0.031539\n",
      ">> Epoch 70 finished \tANN training loss 0.031299\n",
      ">> Epoch 71 finished \tANN training loss 0.031065\n",
      ">> Epoch 72 finished \tANN training loss 0.030831\n",
      ">> Epoch 73 finished \tANN training loss 0.030601\n",
      ">> Epoch 74 finished \tANN training loss 0.030378\n",
      ">> Epoch 75 finished \tANN training loss 0.030166\n",
      ">> Epoch 76 finished \tANN training loss 0.029940\n",
      ">> Epoch 77 finished \tANN training loss 0.029720\n",
      ">> Epoch 78 finished \tANN training loss 0.029531\n",
      ">> Epoch 79 finished \tANN training loss 0.029319\n",
      ">> Epoch 80 finished \tANN training loss 0.029115\n",
      ">> Epoch 81 finished \tANN training loss 0.028914\n",
      ">> Epoch 82 finished \tANN training loss 0.028720\n",
      ">> Epoch 83 finished \tANN training loss 0.028528\n",
      ">> Epoch 84 finished \tANN training loss 0.028341\n",
      ">> Epoch 85 finished \tANN training loss 0.028146\n",
      ">> Epoch 86 finished \tANN training loss 0.027971\n",
      ">> Epoch 87 finished \tANN training loss 0.027794\n",
      ">> Epoch 88 finished \tANN training loss 0.027616\n",
      ">> Epoch 89 finished \tANN training loss 0.027441\n",
      ">> Epoch 90 finished \tANN training loss 0.027266\n",
      ">> Epoch 91 finished \tANN training loss 0.027105\n",
      ">> Epoch 92 finished \tANN training loss 0.026939\n",
      ">> Epoch 93 finished \tANN training loss 0.026779\n",
      ">> Epoch 94 finished \tANN training loss 0.026614\n",
      ">> Epoch 95 finished \tANN training loss 0.026462\n",
      ">> Epoch 96 finished \tANN training loss 0.026311\n",
      ">> Epoch 97 finished \tANN training loss 0.026144\n",
      ">> Epoch 98 finished \tANN training loss 0.026006\n",
      ">> Epoch 99 finished \tANN training loss 0.025848\n",
      ">> Epoch 100 finished \tANN training loss 0.025716\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.343541\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.404890\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.423083\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.419620\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.408693\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.393599\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.374745\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.359032\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.342472\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.329100\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.492371\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.483833\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.480885\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.477485\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.473992\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.470789\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.466777\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.463191\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.458820\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.454805\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.074742\n",
      ">> Epoch 2 finished \tANN training loss 0.070716\n",
      ">> Epoch 3 finished \tANN training loss 0.069434\n",
      ">> Epoch 4 finished \tANN training loss 0.068230\n",
      ">> Epoch 5 finished \tANN training loss 0.067057\n",
      ">> Epoch 6 finished \tANN training loss 0.065918\n",
      ">> Epoch 7 finished \tANN training loss 0.064814\n",
      ">> Epoch 8 finished \tANN training loss 0.063715\n",
      ">> Epoch 9 finished \tANN training loss 0.062686\n",
      ">> Epoch 10 finished \tANN training loss 0.061698\n",
      ">> Epoch 11 finished \tANN training loss 0.060673\n",
      ">> Epoch 12 finished \tANN training loss 0.059757\n",
      ">> Epoch 13 finished \tANN training loss 0.058843\n",
      ">> Epoch 14 finished \tANN training loss 0.057939\n",
      ">> Epoch 15 finished \tANN training loss 0.057056\n",
      ">> Epoch 16 finished \tANN training loss 0.056202\n",
      ">> Epoch 17 finished \tANN training loss 0.055381\n",
      ">> Epoch 18 finished \tANN training loss 0.054569\n",
      ">> Epoch 19 finished \tANN training loss 0.053797\n",
      ">> Epoch 20 finished \tANN training loss 0.053009\n",
      ">> Epoch 21 finished \tANN training loss 0.052288\n",
      ">> Epoch 22 finished \tANN training loss 0.051570\n",
      ">> Epoch 23 finished \tANN training loss 0.050842\n",
      ">> Epoch 24 finished \tANN training loss 0.050166\n",
      ">> Epoch 25 finished \tANN training loss 0.049501\n",
      ">> Epoch 26 finished \tANN training loss 0.048836\n",
      ">> Epoch 27 finished \tANN training loss 0.048223\n",
      ">> Epoch 28 finished \tANN training loss 0.047580\n",
      ">> Epoch 29 finished \tANN training loss 0.046998\n",
      ">> Epoch 30 finished \tANN training loss 0.046391\n",
      ">> Epoch 31 finished \tANN training loss 0.045810\n",
      ">> Epoch 32 finished \tANN training loss 0.045249\n",
      ">> Epoch 33 finished \tANN training loss 0.044704\n",
      ">> Epoch 34 finished \tANN training loss 0.044180\n",
      ">> Epoch 35 finished \tANN training loss 0.043664\n",
      ">> Epoch 36 finished \tANN training loss 0.043157\n",
      ">> Epoch 37 finished \tANN training loss 0.042664\n",
      ">> Epoch 38 finished \tANN training loss 0.042181\n",
      ">> Epoch 39 finished \tANN training loss 0.041710\n",
      ">> Epoch 40 finished \tANN training loss 0.041241\n",
      ">> Epoch 41 finished \tANN training loss 0.040802\n",
      ">> Epoch 42 finished \tANN training loss 0.040375\n",
      ">> Epoch 43 finished \tANN training loss 0.039946\n",
      ">> Epoch 44 finished \tANN training loss 0.039511\n",
      ">> Epoch 45 finished \tANN training loss 0.039120\n",
      ">> Epoch 46 finished \tANN training loss 0.038728\n",
      ">> Epoch 47 finished \tANN training loss 0.038355\n",
      ">> Epoch 48 finished \tANN training loss 0.037971\n",
      ">> Epoch 49 finished \tANN training loss 0.037601\n",
      ">> Epoch 50 finished \tANN training loss 0.037254\n",
      ">> Epoch 51 finished \tANN training loss 0.036905\n",
      ">> Epoch 52 finished \tANN training loss 0.036567\n",
      ">> Epoch 53 finished \tANN training loss 0.036244\n",
      ">> Epoch 54 finished \tANN training loss 0.035927\n",
      ">> Epoch 55 finished \tANN training loss 0.035610\n",
      ">> Epoch 56 finished \tANN training loss 0.035302\n",
      ">> Epoch 57 finished \tANN training loss 0.035003\n",
      ">> Epoch 58 finished \tANN training loss 0.034715\n",
      ">> Epoch 59 finished \tANN training loss 0.034441\n",
      ">> Epoch 60 finished \tANN training loss 0.034168\n",
      ">> Epoch 61 finished \tANN training loss 0.033896\n",
      ">> Epoch 62 finished \tANN training loss 0.033619\n",
      ">> Epoch 63 finished \tANN training loss 0.033384\n",
      ">> Epoch 64 finished \tANN training loss 0.033126\n",
      ">> Epoch 65 finished \tANN training loss 0.032890\n",
      ">> Epoch 66 finished \tANN training loss 0.032662\n",
      ">> Epoch 67 finished \tANN training loss 0.032421\n",
      ">> Epoch 68 finished \tANN training loss 0.032211\n",
      ">> Epoch 69 finished \tANN training loss 0.031985\n",
      ">> Epoch 70 finished \tANN training loss 0.031775\n",
      ">> Epoch 71 finished \tANN training loss 0.031562\n",
      ">> Epoch 72 finished \tANN training loss 0.031372\n",
      ">> Epoch 73 finished \tANN training loss 0.031164\n",
      ">> Epoch 74 finished \tANN training loss 0.030979\n",
      ">> Epoch 75 finished \tANN training loss 0.030779\n",
      ">> Epoch 76 finished \tANN training loss 0.030596\n",
      ">> Epoch 77 finished \tANN training loss 0.030432\n",
      ">> Epoch 78 finished \tANN training loss 0.030260\n",
      ">> Epoch 79 finished \tANN training loss 0.030085\n",
      ">> Epoch 80 finished \tANN training loss 0.029922\n",
      ">> Epoch 81 finished \tANN training loss 0.029759\n",
      ">> Epoch 82 finished \tANN training loss 0.029607\n",
      ">> Epoch 83 finished \tANN training loss 0.029441\n",
      ">> Epoch 84 finished \tANN training loss 0.029307\n",
      ">> Epoch 85 finished \tANN training loss 0.029155\n",
      ">> Epoch 86 finished \tANN training loss 0.029012\n",
      ">> Epoch 87 finished \tANN training loss 0.028876\n",
      ">> Epoch 88 finished \tANN training loss 0.028738\n",
      ">> Epoch 89 finished \tANN training loss 0.028608\n",
      ">> Epoch 90 finished \tANN training loss 0.028478\n",
      ">> Epoch 91 finished \tANN training loss 0.028346\n",
      ">> Epoch 92 finished \tANN training loss 0.028226\n",
      ">> Epoch 93 finished \tANN training loss 0.028102\n",
      ">> Epoch 94 finished \tANN training loss 0.027988\n",
      ">> Epoch 95 finished \tANN training loss 0.027869\n",
      ">> Epoch 96 finished \tANN training loss 0.027750\n",
      ">> Epoch 97 finished \tANN training loss 0.027639\n",
      ">> Epoch 98 finished \tANN training loss 0.027534\n",
      ">> Epoch 99 finished \tANN training loss 0.027423\n",
      ">> Epoch 100 finished \tANN training loss 0.027314\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.383446\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.418376\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.440833\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.441529\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.431759\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.419947\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.404878\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.388480\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.372776\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.355610\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.448543\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.436384\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.434140\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.431776\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.428821\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.425959\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.422981\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.419804\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.416434\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.413074\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.062113\n",
      ">> Epoch 2 finished \tANN training loss 0.059404\n",
      ">> Epoch 3 finished \tANN training loss 0.058653\n",
      ">> Epoch 4 finished \tANN training loss 0.058006\n",
      ">> Epoch 5 finished \tANN training loss 0.057315\n",
      ">> Epoch 6 finished \tANN training loss 0.056634\n",
      ">> Epoch 7 finished \tANN training loss 0.055977\n",
      ">> Epoch 8 finished \tANN training loss 0.055339\n",
      ">> Epoch 9 finished \tANN training loss 0.054689\n",
      ">> Epoch 10 finished \tANN training loss 0.054065\n",
      ">> Epoch 11 finished \tANN training loss 0.053434\n",
      ">> Epoch 12 finished \tANN training loss 0.052849\n",
      ">> Epoch 13 finished \tANN training loss 0.052224\n",
      ">> Epoch 14 finished \tANN training loss 0.051659\n",
      ">> Epoch 15 finished \tANN training loss 0.051072\n",
      ">> Epoch 16 finished \tANN training loss 0.050510\n",
      ">> Epoch 17 finished \tANN training loss 0.049937\n",
      ">> Epoch 18 finished \tANN training loss 0.049397\n",
      ">> Epoch 19 finished \tANN training loss 0.048849\n",
      ">> Epoch 20 finished \tANN training loss 0.048305\n",
      ">> Epoch 21 finished \tANN training loss 0.047784\n",
      ">> Epoch 22 finished \tANN training loss 0.047258\n",
      ">> Epoch 23 finished \tANN training loss 0.046746\n",
      ">> Epoch 24 finished \tANN training loss 0.046242\n",
      ">> Epoch 25 finished \tANN training loss 0.045743\n",
      ">> Epoch 26 finished \tANN training loss 0.045241\n",
      ">> Epoch 27 finished \tANN training loss 0.044754\n",
      ">> Epoch 28 finished \tANN training loss 0.044288\n",
      ">> Epoch 29 finished \tANN training loss 0.043814\n",
      ">> Epoch 30 finished \tANN training loss 0.043353\n",
      ">> Epoch 31 finished \tANN training loss 0.042888\n",
      ">> Epoch 32 finished \tANN training loss 0.042431\n",
      ">> Epoch 33 finished \tANN training loss 0.041975\n",
      ">> Epoch 34 finished \tANN training loss 0.041554\n",
      ">> Epoch 35 finished \tANN training loss 0.041118\n",
      ">> Epoch 36 finished \tANN training loss 0.040691\n",
      ">> Epoch 37 finished \tANN training loss 0.040262\n",
      ">> Epoch 38 finished \tANN training loss 0.039850\n",
      ">> Epoch 39 finished \tANN training loss 0.039452\n",
      ">> Epoch 40 finished \tANN training loss 0.039039\n",
      ">> Epoch 41 finished \tANN training loss 0.038637\n",
      ">> Epoch 42 finished \tANN training loss 0.038250\n",
      ">> Epoch 43 finished \tANN training loss 0.037862\n",
      ">> Epoch 44 finished \tANN training loss 0.037472\n",
      ">> Epoch 45 finished \tANN training loss 0.037089\n",
      ">> Epoch 46 finished \tANN training loss 0.036731\n",
      ">> Epoch 47 finished \tANN training loss 0.036355\n",
      ">> Epoch 48 finished \tANN training loss 0.035983\n",
      ">> Epoch 49 finished \tANN training loss 0.035646\n",
      ">> Epoch 50 finished \tANN training loss 0.035285\n",
      ">> Epoch 51 finished \tANN training loss 0.034945\n",
      ">> Epoch 52 finished \tANN training loss 0.034608\n",
      ">> Epoch 53 finished \tANN training loss 0.034272\n",
      ">> Epoch 54 finished \tANN training loss 0.033941\n",
      ">> Epoch 55 finished \tANN training loss 0.033615\n",
      ">> Epoch 56 finished \tANN training loss 0.033296\n",
      ">> Epoch 57 finished \tANN training loss 0.032981\n",
      ">> Epoch 58 finished \tANN training loss 0.032670\n",
      ">> Epoch 59 finished \tANN training loss 0.032365\n",
      ">> Epoch 60 finished \tANN training loss 0.032066\n",
      ">> Epoch 61 finished \tANN training loss 0.031769\n",
      ">> Epoch 62 finished \tANN training loss 0.031480\n",
      ">> Epoch 63 finished \tANN training loss 0.031188\n",
      ">> Epoch 64 finished \tANN training loss 0.030885\n",
      ">> Epoch 65 finished \tANN training loss 0.030625\n",
      ">> Epoch 66 finished \tANN training loss 0.030354\n",
      ">> Epoch 67 finished \tANN training loss 0.030082\n",
      ">> Epoch 68 finished \tANN training loss 0.029819\n",
      ">> Epoch 69 finished \tANN training loss 0.029556\n",
      ">> Epoch 70 finished \tANN training loss 0.029288\n",
      ">> Epoch 71 finished \tANN training loss 0.029047\n",
      ">> Epoch 72 finished \tANN training loss 0.028800\n",
      ">> Epoch 73 finished \tANN training loss 0.028547\n",
      ">> Epoch 74 finished \tANN training loss 0.028313\n",
      ">> Epoch 75 finished \tANN training loss 0.028079\n",
      ">> Epoch 76 finished \tANN training loss 0.027846\n",
      ">> Epoch 77 finished \tANN training loss 0.027617\n",
      ">> Epoch 78 finished \tANN training loss 0.027396\n",
      ">> Epoch 79 finished \tANN training loss 0.027181\n",
      ">> Epoch 80 finished \tANN training loss 0.026963\n",
      ">> Epoch 81 finished \tANN training loss 0.026749\n",
      ">> Epoch 82 finished \tANN training loss 0.026526\n",
      ">> Epoch 83 finished \tANN training loss 0.026332\n",
      ">> Epoch 84 finished \tANN training loss 0.026129\n",
      ">> Epoch 85 finished \tANN training loss 0.025928\n",
      ">> Epoch 86 finished \tANN training loss 0.025744\n",
      ">> Epoch 87 finished \tANN training loss 0.025544\n",
      ">> Epoch 88 finished \tANN training loss 0.025364\n",
      ">> Epoch 89 finished \tANN training loss 0.025176\n",
      ">> Epoch 90 finished \tANN training loss 0.025003\n",
      ">> Epoch 91 finished \tANN training loss 0.024820\n",
      ">> Epoch 92 finished \tANN training loss 0.024647\n",
      ">> Epoch 93 finished \tANN training loss 0.024472\n",
      ">> Epoch 94 finished \tANN training loss 0.024299\n",
      ">> Epoch 95 finished \tANN training loss 0.024135\n",
      ">> Epoch 96 finished \tANN training loss 0.023971\n",
      ">> Epoch 97 finished \tANN training loss 0.023816\n",
      ">> Epoch 98 finished \tANN training loss 0.023652\n",
      ">> Epoch 99 finished \tANN training loss 0.023498\n",
      ">> Epoch 100 finished \tANN training loss 0.023354\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.331227\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.376877\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.394006\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.394361\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.381178\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.367248\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.352615\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.337063\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.321010\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.306212\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.522344\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.513639\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.510878\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.507801\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.504483\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.500986\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.497300\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.493749\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.489692\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.485370\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 1 finished \tANN training loss 0.070358\n",
      ">> Epoch 2 finished \tANN training loss 0.061188\n",
      ">> Epoch 3 finished \tANN training loss 0.060040\n",
      ">> Epoch 4 finished \tANN training loss 0.058932\n",
      ">> Epoch 5 finished \tANN training loss 0.057851\n",
      ">> Epoch 6 finished \tANN training loss 0.056825\n",
      ">> Epoch 7 finished \tANN training loss 0.055806\n",
      ">> Epoch 8 finished \tANN training loss 0.054817\n",
      ">> Epoch 9 finished \tANN training loss 0.053868\n",
      ">> Epoch 10 finished \tANN training loss 0.052947\n",
      ">> Epoch 11 finished \tANN training loss 0.052035\n",
      ">> Epoch 12 finished \tANN training loss 0.051163\n",
      ">> Epoch 13 finished \tANN training loss 0.050310\n",
      ">> Epoch 14 finished \tANN training loss 0.049499\n",
      ">> Epoch 15 finished \tANN training loss 0.048681\n",
      ">> Epoch 16 finished \tANN training loss 0.047887\n",
      ">> Epoch 17 finished \tANN training loss 0.047128\n",
      ">> Epoch 18 finished \tANN training loss 0.046388\n",
      ">> Epoch 19 finished \tANN training loss 0.045688\n",
      ">> Epoch 20 finished \tANN training loss 0.044985\n",
      ">> Epoch 21 finished \tANN training loss 0.044297\n",
      ">> Epoch 22 finished \tANN training loss 0.043642\n",
      ">> Epoch 23 finished \tANN training loss 0.042976\n",
      ">> Epoch 24 finished \tANN training loss 0.042379\n",
      ">> Epoch 25 finished \tANN training loss 0.041753\n",
      ">> Epoch 26 finished \tANN training loss 0.041153\n",
      ">> Epoch 27 finished \tANN training loss 0.040584\n",
      ">> Epoch 28 finished \tANN training loss 0.040011\n",
      ">> Epoch 29 finished \tANN training loss 0.039479\n",
      ">> Epoch 30 finished \tANN training loss 0.038944\n",
      ">> Epoch 31 finished \tANN training loss 0.038425\n",
      ">> Epoch 32 finished \tANN training loss 0.037918\n",
      ">> Epoch 33 finished \tANN training loss 0.037434\n",
      ">> Epoch 34 finished \tANN training loss 0.036945\n",
      ">> Epoch 35 finished \tANN training loss 0.036498\n",
      ">> Epoch 36 finished \tANN training loss 0.036030\n",
      ">> Epoch 37 finished \tANN training loss 0.035593\n",
      ">> Epoch 38 finished \tANN training loss 0.035166\n",
      ">> Epoch 39 finished \tANN training loss 0.034748\n",
      ">> Epoch 40 finished \tANN training loss 0.034345\n",
      ">> Epoch 41 finished \tANN training loss 0.033956\n",
      ">> Epoch 42 finished \tANN training loss 0.033571\n",
      ">> Epoch 43 finished \tANN training loss 0.033200\n",
      ">> Epoch 44 finished \tANN training loss 0.032834\n",
      ">> Epoch 45 finished \tANN training loss 0.032490\n",
      ">> Epoch 46 finished \tANN training loss 0.032142\n",
      ">> Epoch 47 finished \tANN training loss 0.031806\n",
      ">> Epoch 48 finished \tANN training loss 0.031482\n",
      ">> Epoch 49 finished \tANN training loss 0.031179\n",
      ">> Epoch 50 finished \tANN training loss 0.030872\n",
      ">> Epoch 51 finished \tANN training loss 0.030579\n",
      ">> Epoch 52 finished \tANN training loss 0.030293\n",
      ">> Epoch 53 finished \tANN training loss 0.030011\n",
      ">> Epoch 54 finished \tANN training loss 0.029734\n",
      ">> Epoch 55 finished \tANN training loss 0.029470\n",
      ">> Epoch 56 finished \tANN training loss 0.029215\n",
      ">> Epoch 57 finished \tANN training loss 0.028962\n",
      ">> Epoch 58 finished \tANN training loss 0.028721\n",
      ">> Epoch 59 finished \tANN training loss 0.028480\n",
      ">> Epoch 60 finished \tANN training loss 0.028251\n",
      ">> Epoch 61 finished \tANN training loss 0.028032\n",
      ">> Epoch 62 finished \tANN training loss 0.027808\n",
      ">> Epoch 63 finished \tANN training loss 0.027599\n",
      ">> Epoch 64 finished \tANN training loss 0.027397\n",
      ">> Epoch 65 finished \tANN training loss 0.027195\n",
      ">> Epoch 66 finished \tANN training loss 0.027001\n",
      ">> Epoch 67 finished \tANN training loss 0.026801\n",
      ">> Epoch 68 finished \tANN training loss 0.026626\n",
      ">> Epoch 69 finished \tANN training loss 0.026447\n",
      ">> Epoch 70 finished \tANN training loss 0.026269\n",
      ">> Epoch 71 finished \tANN training loss 0.026100\n",
      ">> Epoch 72 finished \tANN training loss 0.025939\n",
      ">> Epoch 73 finished \tANN training loss 0.025777\n",
      ">> Epoch 74 finished \tANN training loss 0.025615\n",
      ">> Epoch 75 finished \tANN training loss 0.025467\n",
      ">> Epoch 76 finished \tANN training loss 0.025315\n",
      ">> Epoch 77 finished \tANN training loss 0.025170\n",
      ">> Epoch 78 finished \tANN training loss 0.025032\n",
      ">> Epoch 79 finished \tANN training loss 0.024892\n",
      ">> Epoch 80 finished \tANN training loss 0.024758\n",
      ">> Epoch 81 finished \tANN training loss 0.024633\n",
      ">> Epoch 82 finished \tANN training loss 0.024495\n",
      ">> Epoch 83 finished \tANN training loss 0.024362\n",
      ">> Epoch 84 finished \tANN training loss 0.024256\n",
      ">> Epoch 85 finished \tANN training loss 0.024136\n",
      ">> Epoch 86 finished \tANN training loss 0.024020\n",
      ">> Epoch 87 finished \tANN training loss 0.023905\n",
      ">> Epoch 88 finished \tANN training loss 0.023794\n",
      ">> Epoch 89 finished \tANN training loss 0.023687\n",
      ">> Epoch 90 finished \tANN training loss 0.023578\n",
      ">> Epoch 91 finished \tANN training loss 0.023468\n",
      ">> Epoch 92 finished \tANN training loss 0.023372\n",
      ">> Epoch 93 finished \tANN training loss 0.023269\n",
      ">> Epoch 94 finished \tANN training loss 0.023174\n",
      ">> Epoch 95 finished \tANN training loss 0.023070\n",
      ">> Epoch 96 finished \tANN training loss 0.022982\n",
      ">> Epoch 97 finished \tANN training loss 0.022888\n",
      ">> Epoch 98 finished \tANN training loss 0.022798\n",
      ">> Epoch 99 finished \tANN training loss 0.022710\n",
      ">> Epoch 100 finished \tANN training loss 0.022624\n",
      "[END] Fine tuning step\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n",
      "self._get_param_names()= []\n",
      "init_signature.parameters.values()= ['self', 'kwargs']\n"
     ]
    }
   ],
   "source": [
    "# model disponible at https://github.com/albertbup/deep-belief-network/blob/master/README.md\n",
    "import warnings\n",
    "from src import fit_predict_models as fpm\n",
    "\n",
    "from dbn.models import SupervisedDBNRegression\n",
    "import sys\n",
    "print(sys.version)\n",
    "print()\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "model_execs = 10\n",
    "data_title = 'dbn'\n",
    "\n",
    "\n",
    "parameters = {'time_window':[12]}\n",
    "model = SupervisedDBNRegression(\n",
    "                    hidden_layers_structure=[100, 200],\n",
    "                    learning_rate_rbm=[0.01,0.001],\n",
    "                    learning_rate=[0.01,0.001],\n",
    "                    n_epochs_rbm=20,\n",
    "                    n_iter_backprop=200,\n",
    "                    batch_size=16,\n",
    "                    activation_function='relu',  verbose=False)\n",
    "\n",
    "fpm.train_sklearn(model_execs, data_title, parameters, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}